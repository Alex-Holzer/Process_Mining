{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Union, List, Dict, Tuple, Optional\n",
    "from pyspark.sql.column import Column\n",
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    (\"A1\", \"2022-01-23 08:00\", \"Antrag Start\", \"Start\"),\n",
    "    (\"A1\", \"2022-01-23 08:10\", \"Fristablauf\", \"BU\"),\n",
    "    (\"A1\", \"2022-01-23 08:15\", \"Vorschlag\", None),\n",
    "    (\"A2\", \"2022-01-24 10:00\", \"Antrag Start\", None),\n",
    "    (\"A2\", \"2022-01-23 10:20\", \"Sync\", \"Sync Tarif\"),\n",
    "]\n",
    "\n",
    "columns = [\"CASE_KEY\", \"Datum\", \"Funktion\", \"Tarifname\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinct values in a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_values_with_first_appearance(\n",
    "    df: DataFrame, first_date_col: str, columns_to_show: Union[str, List[str]]\n",
    ") -> Dict[str, DataFrame]:\n",
    "    \"\"\"\n",
    "    Compute distinct values for each column in the DataFrame along with the first appearance date of each value.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        first_date_col (str): The name of the column representing the date of first appearance.\n",
    "        columns_to_show (Union[str, List[str]]): Columns to display. It can be 'all', a list of column names, or a single column name.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, DataFrame]: A dictionary with column names as keys and DataFrames as values containing distinct values and their first appearance dates.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `columns_to_show` is neither 'all', a column name, nor a list of column names.\n",
    "\n",
    "    Examples:\n",
    "        >>> df = spark.createDataFrame([\n",
    "        ...     (\"A1\", \"2022-01-23 08:00\", \"Antrag Start\"),\n",
    "        ...     (\"A1\", \"2022-01-23 08:10\", \"Fristablauf\"),\n",
    "        ...     (\"A2\", \"2022-01-23 10:00\", \"Antrag Start\"),\n",
    "        ...     (\"A2\", \"2022-01-23 10:20\", \"Sync\")\n",
    "        ... ], [\"CASE_KEY\", \"Datum\", \"Funktion\"])\n",
    "        >>> result = distinct_values_with_first_appearance(df, \"Datum\", \"all\")\n",
    "        >>> result[\"CASE_KEY\"].show()\n",
    "        >>> result[\"Funktion\"].show()\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(columns_to_show, (str, list)):\n",
    "        raise ValueError(\n",
    "            \"`columns_to_show` must be 'all', a column name, or a list of column names.\"\n",
    "        )\n",
    "\n",
    "    columns = [col for col in df.columns if col != first_date_col]\n",
    "\n",
    "    result = {}\n",
    "    for column in columns:\n",
    "        distinct_df = df.select(column).distinct()\n",
    "        first_date_df = df.groupBy(column).agg(\n",
    "            F.min(first_date_col).alias(f\"{column}_first_date\")\n",
    "        )\n",
    "        distinct_df = distinct_df.join(first_date_df, on=column, how=\"left\")\n",
    "        result[column] = distinct_df\n",
    "\n",
    "    if columns_to_show == \"all\":\n",
    "        columns_to_display = columns\n",
    "    elif isinstance(columns_to_show, str):\n",
    "        columns_to_display = [columns_to_show]\n",
    "    else:\n",
    "        columns_to_display = columns_to_show\n",
    "\n",
    "    for column in columns_to_display:\n",
    "        if column in result:\n",
    "            print(f\"Column: {column}\")\n",
    "            result[column].show()\n",
    "        else:\n",
    "            raise ValueError(f\"Column `{column}` not found in the DataFrame.\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: CASE_KEY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|CASE_KEY|CASE_KEY_first_date|\n",
      "+--------+-------------------+\n",
      "|      A1|   2022-01-23 08:00|\n",
      "|      A2|   2022-01-23 10:20|\n",
      "+--------+-------------------+\n",
      "\n",
      "Column: Funktion\n",
      "+------------+-------------------+\n",
      "|    Funktion|Funktion_first_date|\n",
      "+------------+-------------------+\n",
      "|Antrag Start|   2022-01-23 08:00|\n",
      "| Fristablauf|   2022-01-23 08:10|\n",
      "|   Vorschlag|   2022-01-23 08:15|\n",
      "|        Sync|   2022-01-23 10:20|\n",
      "+------------+-------------------+\n",
      "\n",
      "Column: Tarifname\n",
      "+----------+--------------------+\n",
      "| Tarifname|Tarifname_first_date|\n",
      "+----------+--------------------+\n",
      "|     Start|    2022-01-23 08:00|\n",
      "|        BU|    2022-01-23 08:10|\n",
      "|      null|                null|\n",
      "|Sync Tarif|    2022-01-23 10:20|\n",
      "+----------+--------------------+\n",
      "\n",
      "+--------+-------------------+\n",
      "|CASE_KEY|CASE_KEY_first_date|\n",
      "+--------+-------------------+\n",
      "|      A1|   2022-01-23 08:00|\n",
      "|      A2|   2022-01-23 10:20|\n",
      "+--------+-------------------+\n",
      "\n",
      "+------------+-------------------+\n",
      "|    Funktion|Funktion_first_date|\n",
      "+------------+-------------------+\n",
      "|Antrag Start|   2022-01-23 08:00|\n",
      "| Fristablauf|   2022-01-23 08:10|\n",
      "|   Vorschlag|   2022-01-23 08:15|\n",
      "|        Sync|   2022-01-23 10:20|\n",
      "+------------+-------------------+\n",
      "\n",
      "+----------+--------------------+\n",
      "| Tarifname|Tarifname_first_date|\n",
      "+----------+--------------------+\n",
      "|     Start|    2022-01-23 08:00|\n",
      "|        BU|    2022-01-23 08:10|\n",
      "|      null|                null|\n",
      "|Sync Tarif|    2022-01-23 10:20|\n",
      "+----------+--------------------+\n",
      "\n",
      "Column: CASE_KEY\n",
      "+--------+-------------------+\n",
      "|CASE_KEY|CASE_KEY_first_date|\n",
      "+--------+-------------------+\n",
      "|      A1|   2022-01-23 08:00|\n",
      "|      A2|   2022-01-23 10:20|\n",
      "+--------+-------------------+\n",
      "\n",
      "Column: Funktion\n",
      "+------------+-------------------+\n",
      "|    Funktion|Funktion_first_date|\n",
      "+------------+-------------------+\n",
      "|Antrag Start|   2022-01-23 08:00|\n",
      "| Fristablauf|   2022-01-23 08:10|\n",
      "|   Vorschlag|   2022-01-23 08:15|\n",
      "|        Sync|   2022-01-23 10:20|\n",
      "+------------+-------------------+\n",
      "\n",
      "+--------+-------------------+\n",
      "|CASE_KEY|CASE_KEY_first_date|\n",
      "+--------+-------------------+\n",
      "|      A1|   2022-01-23 08:00|\n",
      "|      A2|   2022-01-23 10:20|\n",
      "+--------+-------------------+\n",
      "\n",
      "+------------+-------------------+\n",
      "|    Funktion|Funktion_first_date|\n",
      "+------------+-------------------+\n",
      "|Antrag Start|   2022-01-23 08:00|\n",
      "| Fristablauf|   2022-01-23 08:10|\n",
      "|   Vorschlag|   2022-01-23 08:15|\n",
      "|        Sync|   2022-01-23 10:20|\n",
      "+------------+-------------------+\n",
      "\n",
      "Column: Funktion\n",
      "+------------+-------------------+\n",
      "|    Funktion|Funktion_first_date|\n",
      "+------------+-------------------+\n",
      "|Antrag Start|   2022-01-23 08:00|\n",
      "| Fristablauf|   2022-01-23 08:10|\n",
      "|   Vorschlag|   2022-01-23 08:15|\n",
      "|        Sync|   2022-01-23 10:20|\n",
      "+------------+-------------------+\n",
      "\n",
      "+------------+-------------------+\n",
      "|    Funktion|Funktion_first_date|\n",
      "+------------+-------------------+\n",
      "|Antrag Start|   2022-01-23 08:00|\n",
      "| Fristablauf|   2022-01-23 08:10|\n",
      "|   Vorschlag|   2022-01-23 08:15|\n",
      "|        Sync|   2022-01-23 10:20|\n",
      "+------------+-------------------+\n",
      "\n",
      "Invalid value for columns_to_show. It should be 'all', a column name, or a list of column names.\n",
      "Column: CASE_KEY\n",
      "+--------+-------------------+\n",
      "|CASE_KEY|CASE_KEY_first_date|\n",
      "+--------+-------------------+\n",
      "|      A1|   2022-01-23 08:00|\n",
      "|      A2|   2022-01-23 10:20|\n",
      "+--------+-------------------+\n",
      "\n",
      "Column: Funktion\n",
      "+------------+-------------------+\n",
      "|    Funktion|Funktion_first_date|\n",
      "+------------+-------------------+\n",
      "|Antrag Start|   2022-01-23 08:00|\n",
      "| Fristablauf|   2022-01-23 08:10|\n",
      "|   Vorschlag|   2022-01-23 08:15|\n",
      "|        Sync|   2022-01-23 10:20|\n",
      "+------------+-------------------+\n",
      "\n",
      "Number of distinct CASE_KEY values: 2\n",
      "Number of distinct Funktion values: 4\n",
      "Column: CASE_KEY\n",
      "+--------+-------------------+\n",
      "|CASE_KEY|CASE_KEY_first_date|\n",
      "+--------+-------------------+\n",
      "|      A1|   2022-01-23 08:00|\n",
      "|      A2|   2022-01-23 10:20|\n",
      "+--------+-------------------+\n",
      "\n",
      "Column: Funktion\n",
      "+------------+-------------------+\n",
      "|    Funktion|Funktion_first_date|\n",
      "+------------+-------------------+\n",
      "|Antrag Start|   2022-01-23 08:00|\n",
      "| Fristablauf|   2022-01-23 08:10|\n",
      "|   Vorschlag|   2022-01-23 08:15|\n",
      "|        Sync|   2022-01-23 10:20|\n",
      "+------------+-------------------+\n",
      "\n",
      "Column: Tarifname\n",
      "+----------+--------------------+\n",
      "| Tarifname|Tarifname_first_date|\n",
      "+----------+--------------------+\n",
      "|     Start|    2022-01-23 08:00|\n",
      "|        BU|    2022-01-23 08:10|\n",
      "|      null|                null|\n",
      "|Sync Tarif|    2022-01-23 10:20|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and show all columns\n",
    "result = distinct_values_with_first_appearance(df, \"Datum\", \"all\")\n",
    "\n",
    "# Accessing individual results\n",
    "result[\"CASE_KEY\"].show()\n",
    "result[\"Funktion\"].show()\n",
    "result[\"Tarifname\"].show()\n",
    "\n",
    "# Specify a list of columns to show\n",
    "columns_to_display = [\"CASE_KEY\", \"Funktion\"]\n",
    "\n",
    "# Call the function and show the specified columns\n",
    "result = distinct_values_with_first_appearance(df, \"Datum\", columns_to_display)\n",
    "\n",
    "# Accessing individual results\n",
    "result[\"CASE_KEY\"].show()\n",
    "result[\"Funktion\"].show()\n",
    "\n",
    "\n",
    "# Specify a single column to show\n",
    "column_to_display = \"Funktion\"\n",
    "\n",
    "# Call the function and show the specified column\n",
    "result = distinct_values_with_first_appearance(df, \"Datum\", column_to_display)\n",
    "\n",
    "# Accessing the result\n",
    "result[\"Funktion\"].show()\n",
    "\n",
    "\n",
    "try:\n",
    "    # Call the function with an invalid column name\n",
    "    distinct_values_with_first_appearance(df, \"Datum\", \"InvalidColumn\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    # Call the function with an invalid type for columns_to_show\n",
    "    distinct_values_with_first_appearance(df, \"Datum\", 123)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# Call the function and return the results\n",
    "result = distinct_values_with_first_appearance(\n",
    "    df, \"Datum\", [\"CASE_KEY\", \"Funktion\"])\n",
    "\n",
    "# Further processing on the returned DataFrames\n",
    "# For example, count the number of distinct values for each column\n",
    "case_key_count = result[\"CASE_KEY\"].count()\n",
    "funktion_count = result[\"Funktion\"].count()\n",
    "\n",
    "print(f\"Number of distinct CASE_KEY values: {case_key_count}\")\n",
    "print(f\"Number of distinct Funktion values: {funktion_count}\")\n",
    "\n",
    "# Show all columns in the result at once using a loop\n",
    "for column, col_df in result.items():\n",
    "    print(f\"Column: {column}\")\n",
    "    col_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_values(\n",
    "    df: DataFrame, columns_to_show: Union[str, List[str]]\n",
    ") -> Dict[str, DataFrame]:\n",
    "    \"\"\"\n",
    "    Compute distinct values for each column in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        columns_to_show (Union[str, List[str]]): Columns to display. It can be 'all', a list of column names, or a single column name.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, DataFrame]: A dictionary with column names as keys and DataFrames as values containing distinct values.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `columns_to_show` is neither 'all', a column name, nor a list of column names.\n",
    "\n",
    "    Examples:\n",
    "        >>> df = spark.createDataFrame([\n",
    "        ...     (\"A1\", \"2022-01-23 08:00\", \"Antrag Start\", \"Start\"),\n",
    "        ...     (\"A1\", \"2022-01-23 08:10\", \"Fristablauf ext.\", \"BU\"),\n",
    "        ...     (\"A1\", \"2022-01-23 08:15\", \"Vorschlag\", None),\n",
    "        ...     (\"A2\", \"2022-01-23 10:00\", \"Antrag Start\", None),\n",
    "        ...     (\"A2\", \"2022-01-23 10:20\", \"Sync\", \"Sync Tarif\")\n",
    "        ... ], [\"CASE_KEY\", \"Datum\", \"Funktion\", \"Tarifname\"])\n",
    "        >>> result = distinct_values(df, \"all\")\n",
    "        >>> result[\"CASE_KEY\"].show()\n",
    "        >>> result[\"Funktion\"].show()\n",
    "        >>> result[\"Tarifname\"].show()\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(columns_to_show, (str, list)):\n",
    "        raise ValueError(\n",
    "            \"`columns_to_show` must be 'all', a column name, or a list of column names.\"\n",
    "        )\n",
    "\n",
    "    result = {}\n",
    "    columns = df.columns\n",
    "\n",
    "    for column in columns:\n",
    "        distinct_df = df.select(column).distinct()\n",
    "        result[column] = distinct_df\n",
    "\n",
    "    if columns_to_show == \"all\":\n",
    "        columns_to_display = columns\n",
    "    elif isinstance(columns_to_show, str):\n",
    "        columns_to_display = [columns_to_show]\n",
    "    else:\n",
    "        columns_to_display = columns_to_show\n",
    "\n",
    "    for column in columns_to_display:\n",
    "        if column in result:\n",
    "            print(f\"Column: {column}\")\n",
    "            result[column].show()\n",
    "        else:\n",
    "            raise ValueError(f\"Column `{column}` not found in the DataFrame.\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|CASE_KEY|\n",
      "+--------+\n",
      "|      A1|\n",
      "|      A2|\n",
      "+--------+\n",
      "\n",
      "+------------+\n",
      "|    Funktion|\n",
      "+------------+\n",
      "|Antrag Start|\n",
      "| Fristablauf|\n",
      "|   Vorschlag|\n",
      "|        Sync|\n",
      "+------------+\n",
      "\n",
      "+----------+\n",
      "| Tarifname|\n",
      "+----------+\n",
      "|     Start|\n",
      "|        BU|\n",
      "|      null|\n",
      "|Sync Tarif|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and show all columns\n",
    "result = distinct_values(df, \"all\")\n",
    "\n",
    "# Accessing individual results\n",
    "result[\"CASE_KEY\"].show()\n",
    "result[\"Funktion\"].show()\n",
    "result[\"Tarifname\"].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_value_distribution_over_time(\n",
    "    df: DataFrame,\n",
    "    date_col: str,\n",
    "    columns: Union[str, List[str]] = \"all\",\n",
    "    timeframe: str = \"day\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the distribution of null values over time for specified columns with different timeframes.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        date_col (str): The name of the date column.\n",
    "        columns (Union[str, List[str]]): The column(s) to check for null values. It can be 'all', a list of column names, or a single column name.\n",
    "        timeframe (str): The timeframe for aggregation ('year', 'month', 'day').\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame showing the distribution of null values over time.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `timeframe` is not one of 'year', 'month', or 'day'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure columns is a list of relevant columns\n",
    "    if columns == \"all\":\n",
    "        columns = [col for col in df.columns if col != date_col]\n",
    "    elif isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "\n",
    "    # Define the date format based on the timeframe\n",
    "    if timeframe == \"year\":\n",
    "        df = df.withColumn(\"timeframe\", F.year(F.col(date_col)).cast(\"string\"))\n",
    "    elif timeframe == \"month\":\n",
    "        df = df.withColumn(\"timeframe\", F.date_format(F.col(date_col), \"yyyy-MM\"))\n",
    "    elif timeframe == \"day\":\n",
    "        df = df.withColumn(\"timeframe\", F.date_format(F.col(date_col), \"yyyy-MM-dd\"))\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid value for timeframe. It should be 'year', 'month', or 'day'.\"\n",
    "        )\n",
    "\n",
    "    # Create a DataFrame with counts of null and non-null values for each column over time\n",
    "    null_distributions = []\n",
    "    for column in columns:\n",
    "        null_count = df.withColumn(\n",
    "            f\"{column}_is_null\", F.col(column).isNull().cast(\"int\")\n",
    "        )\n",
    "\n",
    "        # Group by timeframe and aggregate counts of null and non-null values\n",
    "        null_distribution = null_count.groupBy(\"timeframe\").agg(\n",
    "            F.sum(f\"{column}_is_null\").alias(f\"{column}_null_count\"),\n",
    "            F.count(f\"{column}_is_null\").alias(f\"{column}_total_count\"),\n",
    "            (F.count(f\"{column}_is_null\") - F.sum(f\"{column}_is_null\")).alias(\n",
    "                f\"{column}_non_null_count\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        null_distributions.append(null_distribution)\n",
    "\n",
    "    # Join all the null distributions on the timeframe column\n",
    "    result_df = null_distributions[0]\n",
    "    for dist_df in null_distributions[1:]:\n",
    "        result_df = result_df.join(dist_df, on=\"timeframe\", how=\"outer\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------------------+------------------------+\n",
      "|timeframe|Tarifname_null_count|Tarifname_total_count|Tarifname_non_null_count|\n",
      "+---------+--------------------+---------------------+------------------------+\n",
      "|  2022-01|                   2|                    5|                       3|\n",
      "+---------+--------------------+---------------------+------------------------+\n",
      "\n",
      "+----------+-------------------+--------------------+-----------------------+--------------------+---------------------+------------------------+\n",
      "| timeframe|Funktion_null_count|Funktion_total_count|Funktion_non_null_count|Tarifname_null_count|Tarifname_total_count|Tarifname_non_null_count|\n",
      "+----------+-------------------+--------------------+-----------------------+--------------------+---------------------+------------------------+\n",
      "|2022-01-23|                  0|                   4|                      4|                   1|                    4|                       3|\n",
      "|2022-01-24|                  0|                   1|                      1|                   1|                    1|                       0|\n",
      "+----------+-------------------+--------------------+-----------------------+--------------------+---------------------+------------------------+\n",
      "\n",
      "+---------+-------------------+--------------------+-----------------------+-------------------+--------------------+-----------------------+--------------------+---------------------+------------------------+\n",
      "|timeframe|CASE_KEY_null_count|CASE_KEY_total_count|CASE_KEY_non_null_count|Funktion_null_count|Funktion_total_count|Funktion_non_null_count|Tarifname_null_count|Tarifname_total_count|Tarifname_non_null_count|\n",
      "+---------+-------------------+--------------------+-----------------------+-------------------+--------------------+-----------------------+--------------------+---------------------+------------------------+\n",
      "|     2022|                  0|                   5|                      5|                  0|                   5|                      5|                   2|                    5|                       3|\n",
      "+---------+-------------------+--------------------+-----------------------+-------------------+--------------------+-----------------------+--------------------+---------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get null value distribution for the \"Tarifname\" column by day\n",
    "result_df = null_value_distribution_over_time(\n",
    "    df, \"Datum\", \"Tarifname\", timeframe=\"month\"\n",
    ")\n",
    "result_df.show()\n",
    "\n",
    "# Get null value distribution for the \"Funktion\" and \"Tarifname\" columns by month\n",
    "result_df = null_value_distribution_over_time(\n",
    "    df, \"Datum\", [\"Funktion\", \"Tarifname\"], timeframe=\"day\"\n",
    ")\n",
    "result_df.show()\n",
    "\n",
    "# Get null value distribution for the \"CASE_KEY\" and \"Tarifname\" columns by year\n",
    "result_df = null_value_distribution_over_time(df, \"Datum\", \"all\", timeframe=\"year\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename multiple columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df: DataFrame, column_mapping: Dict[str, str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Rename columns of a DataFrame according to a provided mapping dictionary.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame whose columns need to be renamed.\n",
    "        column_mapping (Dict[str, str]): A dictionary where the keys are the current column names\n",
    "                                         and the values are the new column names.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with renamed columns.\n",
    "\n",
    "    Example:\n",
    "        >>> spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "        >>> data = [(\"A1\", \"2022-01-23 08:00:00\", \"Antrag Start\", \"Start\"),\n",
    "        ...         (\"A2\", \"2022-01-23 10:00:00\", \"Antrag Start\", \"Start\")]\n",
    "        >>> columns = [\"CASE_KEY\", \"Datum\", \"Funktion\", \"Tarifname\"]\n",
    "        >>> df = spark.createDataFrame(data, columns)\n",
    "        >>> column_mapping = {\"CASE_KEY\": \"case_id\", \"Datum\": \"timestamp\"}\n",
    "        >>> df_renamed = rename_columns(df, column_mapping)\n",
    "        >>> df_renamed.show()\n",
    "        +-------+-------------------+-------------+---------+\n",
    "        |case_id|          timestamp|     Funktion|Tarifname|\n",
    "        +-------+-------------------+-------------+---------+\n",
    "        |     A1|2022-01-23 08:00:00|Antrag Start|    Start |\n",
    "        |     A2|2022-01-23 10:00:00|Antrag Start|    Start |\n",
    "        +-------+-------------------+-------------+---------+\n",
    "    \"\"\"\n",
    "    # Validate that all keys in column_mapping exist in the DataFrame's columns\n",
    "    for old_name in column_mapping.keys():\n",
    "        if old_name not in df.columns:\n",
    "            raise ValueError(f\"Column '{old_name}' does not exist in the DataFrame.\")\n",
    "\n",
    "    # Rename the columns according to the mapping\n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------------+-----------+\n",
      "|case_id|       timestamp|    function|tariff_name|\n",
      "+-------+----------------+------------+-----------+\n",
      "|     A1|2022-01-23 08:00|Antrag Start|      Start|\n",
      "|     A2|2022-01-23 10:00|Antrag Start|      Start|\n",
      "+-------+----------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"A1\", \"2022-01-23 08:00\", \"Antrag Start\", \"Start\"),\n",
    "        (\"A2\", \"2022-01-23 10:00\", \"Antrag Start\", \"Start\"),\n",
    "    ],\n",
    "    [\"CASE_KEY\", \"Datum\", \"Funktion\", \"Tarifname\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Define the column mapping\n",
    "column_mapping = {\n",
    "    \"CASE_KEY\": \"case_id\",\n",
    "    \"Datum\": \"timestamp\",\n",
    "    \"Funktion\": \"function\",\n",
    "    \"Tarifname\": \"tariff_name\",\n",
    "}\n",
    "\n",
    "# Rename the columns of the DataFrame\n",
    "df_renamed = rename_columns(df, column_mapping)\n",
    "\n",
    "# Show the DataFrame with renamed columns\n",
    "df_renamed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Column values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_column_values(\n",
    "    df: DataFrame,\n",
    "    column_name: str,\n",
    "    value_mapping: Dict[str, str],\n",
    "    conditions: Optional[Dict[str, Column]] = None,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Rename values in a specified column of a DataFrame based on a provided mapping dictionary,\n",
    "    and optionally apply additional conditions to set specific values.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        column_name (str): The name of the column whose values need to be renamed.\n",
    "        value_mapping (Dict[str, str]): A dictionary where the keys are the current values\n",
    "                                        and the values are the new values.\n",
    "        conditions (Optional[Dict[str, Column]]): An optional dictionary where the keys are the values to be set\n",
    "                                                  and the values are the conditions to be met.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with renamed values in the specified column.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql.functions import col\n",
    "        >>> spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "        >>> data = [(\"A1\", \"Start\", \"Tarif1\"), (\"A2\", \"Process\", None), (\"A3\", \"End\", \"Tarif2\")]\n",
    "        >>> columns = [\"CASE_KEY\", \"Funktion\", \"Tarif\"]\n",
    "        >>> df = spark.createDataFrame(data, columns)\n",
    "        >>> value_mapping = {\"Start\": \"Begin\", \"Process\": \"In Progress\", \"End\": \"Complete\"}\n",
    "        >>> conditions = {\"SpecialValue\": (col(\"Funktion\").like(\"%Antrag%\") & (col(\"CASE_KEY\") == \"A1\")),\n",
    "        ...               \"TarifValue\": col(\"Tarif\").isNotNull()}\n",
    "        >>> df_renamed = rename_column_values(df, \"Funktion\", value_mapping, conditions)\n",
    "        >>> df_renamed.show()\n",
    "        +--------+-----------+-------+\n",
    "        |CASE_KEY|   Funktion|  Tarif|\n",
    "        +--------+-----------+-------+\n",
    "        |     A1|SpecialValue| Tarif1|\n",
    "        |     A2| In Progress|   null|\n",
    "        |     A3|   Complete| Tarif2|\n",
    "        +--------+-----------+-------+\n",
    "    \"\"\"\n",
    "    # Start with the initial column\n",
    "    updated_column = F.col(column_name)\n",
    "\n",
    "    # Apply conditions if provided\n",
    "    if conditions:\n",
    "        for new_value, condition in conditions.items():\n",
    "            updated_column = F.when(\n",
    "                condition, new_value).otherwise(updated_column)\n",
    "\n",
    "    # Apply the value mapping\n",
    "    for old_value, new_value in value_mapping.items():\n",
    "        updated_column = F.when(F.col(column_name) == old_value, new_value).otherwise(\n",
    "            updated_column\n",
    "        )\n",
    "\n",
    "    # Update the DataFrame with the renamed values\n",
    "    df = df.withColumn(column_name, updated_column)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------+\n",
      "|CASE_KEY|   Funktion| Tarif|\n",
      "+--------+-----------+------+\n",
      "|      A1|      Begin|Tarif1|\n",
      "|      A2|In Progress|  null|\n",
      "|      A3|   Complete|Tarif2|\n",
      "+--------+-----------+------+\n",
      "\n",
      "+--------+-----------+------+\n",
      "|CASE_KEY|   Funktion| Tarif|\n",
      "+--------+-----------+------+\n",
      "|      A1|      Begin|Tarif1|\n",
      "|      A2|In Progress|  null|\n",
      "|      A3|   Complete|Tarif2|\n",
      "+--------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [(\"A1\", \"Start\", \"Tarif1\"), (\"A2\", \"Process\", None), (\"A3\", \"End\", \"Tarif2\")]\n",
    "columns = [\"CASE_KEY\", \"Funktion\", \"Tarif\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Value mapping dictionary\n",
    "value_mapping = {\"Start\": \"Begin\", \"Process\": \"In Progress\", \"End\": \"Complete\"}\n",
    "\n",
    "# Conditions dictionary\n",
    "conditions = {\n",
    "    \"SpecialValue\": (F.col(\"Funktion\").like(\"%Antrag%\")),\n",
    "    \"TarifValue\": F.col(\"Tarif\").isNotNull(),\n",
    "}\n",
    "\n",
    "# Rename column values with conditions\n",
    "df_renamed_with_conditions = rename_column_values(\n",
    "    df, \"Funktion\", value_mapping, conditions\n",
    ")\n",
    "df_renamed_with_conditions.show()\n",
    "\n",
    "# Rename column values without conditions\n",
    "df_renamed_without_conditions = rename_column_values(df, \"Funktion\", value_mapping)\n",
    "df_renamed_without_conditions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Multiple Dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dataframes(\n",
    "    df1: DataFrame,\n",
    "    df2: DataFrame,\n",
    "    join_keys: Tuple[Union[str, List[str]], Union[str, List[str]]],\n",
    "    join_type: str,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Join two DataFrames on the specified join keys with the chosen join type.\n",
    "\n",
    "    Args:\n",
    "        df1 (DataFrame): The first DataFrame.\n",
    "        df2 (DataFrame): The second DataFrame.\n",
    "        join_keys (Tuple[Union[str, List[str]], Union[str, List[str]]]): A tuple containing the join keys for df1 and df2.\n",
    "        join_type (str): The type of join to perform. Options are 'inner', 'outer', 'left', 'right', 'semi', 'anti', etc.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The joined DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the keys in the second DataFrame are not unique.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> data1 = [(\"A1\", 10), (\"A2\", 20)]\n",
    "        >>> columns1 = [\"CASE_KEY_1\", \"Value1\"]\n",
    "        >>> df1 = spark.createDataFrame(data1, columns1)\n",
    "        >>> data2 = [(\"A1\", \"X\"), (\"A2\", \"Y\")]\n",
    "        >>> columns2 = [\"CASE_KEY_2\", \"Value2\"]\n",
    "        >>> df2 = spark.createDataFrame(data2, columns2)\n",
    "        >>> result = join_dataframes(df1, df2, (\"CASE_KEY_1\", \"CASE_KEY_2\"), \"inner\")\n",
    "        >>> result.show()\n",
    "        +-----------+------+-----------+\n",
    "        |CASE_KEY_1 |Value1| CASE_KEY_2|Value2|\n",
    "        +-----------+------+-----------+\n",
    "        |      A1|    10|      A1|     X|\n",
    "        |      A2|    20|      A2|     Y|\n",
    "        +-----------+------+-----------+\n",
    "    \"\"\"\n",
    "    join_key_df1, join_key_df2 = join_keys\n",
    "\n",
    "    # Convert join keys to lists if they are strings\n",
    "    if isinstance(join_key_df1, str):\n",
    "        join_key_df1 = [join_key_df1]\n",
    "    if isinstance(join_key_df2, str):\n",
    "        join_key_df2 = [join_key_df2]\n",
    "\n",
    "    # Check for uniqueness of join_key in df2\n",
    "    unique_check = df2.groupBy(join_key_df2).count().filter(F.col(\"count\") > 1)\n",
    "    if unique_check.count() > 0:\n",
    "        raise ValueError(\"The join keys in the second DataFrame are not unique.\")\n",
    "\n",
    "    # Resolve potential column name conflicts by renaming columns in df2\n",
    "    df1_cols = set(df1.columns)\n",
    "    df2_cols = set(df2.columns)\n",
    "    common_cols = df1_cols.intersection(df2_cols).difference(\n",
    "        set(join_key_df1).union(set(join_key_df2))\n",
    "    )\n",
    "\n",
    "    for F.col in common_cols:\n",
    "        df2 = df2.withColumnRenamed(F.col, f\"{F.col}_df2\")\n",
    "\n",
    "    # Create join condition\n",
    "    join_condition = [df1[k1] == df2[k2] for k1, k2 in zip(join_key_df1, join_key_df2)]\n",
    "\n",
    "    # Perform the join\n",
    "    joined_df = df1.join(df2, on=join_condition, how=join_type)\n",
    "\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+------+\n",
      "|CASE_KEY_1|Value1|CASE_KEY_2|Value2|\n",
      "+----------+------+----------+------+\n",
      "|        A1|    10|        A1|     X|\n",
      "|        A2|    20|        A2|     Y|\n",
      "+----------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Sample data for the first DataFrame\n",
    "data1 = [(\"A1\", 10), (\"A2\", 20)]\n",
    "columns1 = [\"CASE_KEY_1\", \"Value1\"]\n",
    "\n",
    "# Sample data for the second DataFrame\n",
    "data2 = [(\"A1\", \"X\"), (\"A2\", \"Y\")]\n",
    "columns2 = [\"CASE_KEY_2\", \"Value2\"]\n",
    "\n",
    "# Create DataFrames\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Join DataFrames\n",
    "result = join_dataframes(df1, df2, (\"CASE_KEY_1\", \"CASE_KEY_2\"), \"inner\")\n",
    "\n",
    "# Show the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate Checker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_keys(\n",
    "    df: DataFrame, keys: Union[str, List[str]], show_duplicates: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Check whether the combination of keys is unique in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        keys (Union[str, List[str]]): The key(s) to check for uniqueness. This can be a string for a single key or a list of column names.\n",
    "        show_duplicates (bool): If True, shows the DataFrame with duplicate keys. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        str: \"Test passed!\" if the combination of keys is unique, or \"Test failed: Number of duplicates \" followed by the number of duplicates.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If 'keys' is not a string or a list of strings, or if 'keys' are not columns in the DataFrame.\n",
    "\n",
    "    Examples:\n",
    "        >>> spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "        >>> data = [(\"A1\", \"2022-01-23 08:00\", \"Antrag Start\", \"AT\"),\n",
    "        ...         (\"A1\", \"2022-01-23 08:10\", \"Fristablauf ext.\", \"Signatul BU\"),\n",
    "        ...         (\"A2\", \"2022-01-23 10:00\", \"Antrag Start\", \"\")]\n",
    "        >>> columns = [\"CASE_KEY\", \"Datum\", \"Funktion\", \"Tarifname\"]\n",
    "        >>> df = spark.createDataFrame(data, schema=columns)\n",
    "        >>> result = check_unique_keys(df, [\"CASE_KEY\", \"Datum\"])\n",
    "        >>> print(result)\n",
    "        Test passed!\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate input keys\n",
    "    if isinstance(keys, str):\n",
    "        keys = [keys]\n",
    "    elif not isinstance(keys, list) or not all(isinstance(key, str) for key in keys):\n",
    "        raise ValueError(\"'keys' must be a string or a list of strings\")\n",
    "\n",
    "    # Check if all keys are columns in the DataFrame\n",
    "    missing_keys = [key for key in keys if key not in df.columns]\n",
    "    if missing_keys:\n",
    "        raise ValueError(\n",
    "            f\"The following keys are not columns in the DataFrame: {', '.join(missing_keys)}\"\n",
    "        )\n",
    "\n",
    "    # Group by the keys and count the occurrences\n",
    "    grouped_df = df.groupBy(keys).count()\n",
    "\n",
    "    # Filter the groups with count > 1 to find duplicates\n",
    "    duplicates_df = grouped_df.filter(col(\"count\") > 1)\n",
    "    num_duplicates = duplicates_df.count()\n",
    "\n",
    "    if num_duplicates == 0:\n",
    "        return \"Test passed!\"\n",
    "    else:\n",
    "        if show_duplicates:\n",
    "            duplicates = df.join(duplicates_df.select(keys), on=keys, how=\"inner\")\n",
    "            duplicates.show()\n",
    "        return f\"Test failed: Number of duplicates {num_duplicates}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe(df: DataFrame, conditions: Dict[str, Column]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply multiple conditions to filter a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame to filter.\n",
    "        conditions (Dict[str, Column]): A dictionary where the keys are condition names and the values are PySpark Column conditions.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The filtered DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the conditions is not a PySpark Column.\n",
    "\n",
    "    Examples:\n",
    "        >>> from pyspark.sql.functions import col\n",
    "        >>> data = [(\"A1\", \"2022-01-23 08:00\", \"Antrag Start\", \"AT\"),\n",
    "        ...         (\"A1\", \"2022-01-23 08:10\", \"Fristablauf ext.\", \"Signatul BU\"),\n",
    "        ...         (\"A2\", \"2022-01-23 10:00\", \"Antrag Start\", \"\")]\n",
    "        >>> columns = [\"CASE_KEY\", \"Datum\", \"Funktion\", \"Tarifname\"]\n",
    "        >>> df = spark.createDataFrame(data, schema=columns)\n",
    "        >>> conditions = {\n",
    "        ...     \"Tarifname not empty\": col(\"Tarifname\") != \"\",\n",
    "        ...     \"CASE_KEY A1\": col(\"CASE_KEY\") == \"A1\"\n",
    "        ... }\n",
    "        >>> filtered_df = filter_dataframe(df, conditions)\n",
    "        >>> filtered_df.show()\n",
    "        +--------+-------------------+--------------+-----------+\n",
    "        |CASE_KEY|              Datum|      Funktion|  Tarifname|\n",
    "        +--------+-------------------+--------------+-----------+\n",
    "        |      A1|2022-01-23 08:00:00|  Antrag Start|         AT|\n",
    "        |      A1|2022-01-23 08:10:00|Fristablauf ext.|Signatul BU|\n",
    "        +--------+-------------------+--------------+-----------+\n",
    "    \"\"\"\n",
    "    # Validate that all conditions are PySpark Column objects\n",
    "    if not all(isinstance(condition, Column) for condition in conditions.values()):\n",
    "        raise ValueError(\"All conditions must be PySpark Column objects\")\n",
    "\n",
    "    # Apply each condition to filter the DataFrame\n",
    "    for condition in conditions.values():\n",
    "        df = df.filter(condition)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
