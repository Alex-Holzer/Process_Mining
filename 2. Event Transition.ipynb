{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"Raw/data.csv\", header=True, inferSchema=False, sep=\";\")\n",
    "df = df.withColumn(\"Datum\", F.to_timestamp(\"Datum\", \"d.M.yy h:mm\"))\n",
    "# drop cases with missing case_key\n",
    "df = df.filter(F.col(\"case_key\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_sequence_overview(\n",
    "    df: DataFrame, case_key: str, event: str, timestamp: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the most frequent sequence of events and return the result as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        case_key (str): The column name for the case key.\n",
    "        event (str): The column name for the event.\n",
    "        timestamp (str): The column name for the timestamp.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with the most frequent sequence of events and their percentage.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the specified columns do not exist in the DataFrame.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> from pyspark.sql.functions import col\n",
    "        >>> spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "        >>> data = [(\"A1\", \"2022-01-23 08:00\", \"Antrag Start\", \"AT\"),\n",
    "        ...         (\"A1\", \"2022-01-23 08:10\", \"Fristablauf ext.\", \"Signatul BU\"),\n",
    "        ...         (\"A1\", \"2022-01-23 08:15\", \"Vorschlag\", \"\"),\n",
    "        ...         (\"A2\", \"2022-01-23 10:00\", \"Antrag Start\", \"\"),\n",
    "        ...         (\"A2\", \"2022-01-23 10:20\", \"Sync\", \"\"),\n",
    "        ...         (\"A2\", \"2022-01-23 10:50\", \"Laden\", None),\n",
    "        ...         (\"A2\", \"2022-01-23 11:00\", \"Antrag (VE)\", \"\"),\n",
    "        ...         (\"A2\", \"2022-01-23 11:01\", \"Geloescht\", \"\")]\n",
    "        >>> columns = [\"CASE_KEY\", \"Datum\", \"Funktion\", \"Tarifname\"]\n",
    "        >>> df = spark.createDataFrame(data, schema=columns)\n",
    "        >>> result_df = event_sequence_overview(df, \"CASE_KEY\", \"Funktion\", \"Datum\")\n",
    "        >>> result_df.show(truncate=False)\n",
    "    \"\"\"\n",
    "    # Validate that the specified columns exist in the DataFrame\n",
    "    for col_name in [case_key, event, timestamp]:\n",
    "        if col_name not in df.columns:\n",
    "            raise ValueError(f\"The column '{col_name}' does not exist in the DataFrame\")\n",
    "\n",
    "    # Define the window specification\n",
    "    window = Window.partitionBy(case_key).orderBy(timestamp)\n",
    "\n",
    "    # Create a column for the next event in the sequence\n",
    "    df = df.withColumn(\"next_event\", F.lead(event).over(window))\n",
    "\n",
    "    # Filter out rows where next_event is null\n",
    "    df_event_sequence = df.filter(F.col(\"next_event\").isNotNull())\n",
    "\n",
    "    # Create a column for the event sequence\n",
    "    df_event_sequence = df_event_sequence.withColumn(\n",
    "        \"event_sequence\", F.concat(F.col(event), F.lit(\" -> \"), F.col(\"next_event\"))\n",
    "    )\n",
    "\n",
    "    # Calculate the most frequent event sequences\n",
    "    df_most_frequent_sequence = (\n",
    "        df_event_sequence.groupBy(\"event_sequence\")\n",
    "        .agg(F.countDistinct(case_key).alias(\"count\"))\n",
    "        .orderBy(F.desc(\"count\"))\n",
    "    )\n",
    "\n",
    "    # Calculate the total number of unique case keys\n",
    "    total_case_keys = df.select(case_key).distinct().count()\n",
    "\n",
    "    # Calculate the percentage of each event sequence\n",
    "    df_most_frequent_sequence = df_most_frequent_sequence.withColumn(\n",
    "        \"percentage\", (F.col(\"count\") / total_case_keys) * 100\n",
    "    )\n",
    "\n",
    "    return df_most_frequent_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_case_keys_for_event_sequence(\n",
    "    df: DataFrame, case_key: str, event: str, timestamp: str, sequence: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Find the case keys for a particular sequence of events and return the result as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        case_key (str): The column name for the case key.\n",
    "        event (str): The column name for the event.\n",
    "        timestamp (str): The column name for the timestamp.\n",
    "        sequence (str): The sequence of events.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with the case keys for the sequence.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the specified columns do not exist in the DataFrame.\n",
    "\n",
    "    Example:\n",
    "        >>> from pyspark.sql import SparkSession\n",
    "        >>> spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "        >>> data = [(\"A1\", \"2022-01-23 08:00\", \"Antrag Start\", \"AT\"),\n",
    "        ...         (\"A1\", \"2022-01-23 08:10\", \"Fristablauf ext.\", \"Signatul BU\"),\n",
    "        ...         (\"A1\", \"2022-01-23 08:15\", \"Vorschlag\", \"\"),\n",
    "        ...         (\"A2\", \"2022-01-23 10:00\", \"Antrag Start\", \"\"),\n",
    "        ...         (\"A2\", \"2022-01-23 10:20\", \"Sync\", \"\"),\n",
    "        ...         (\"A2\", \"2022-01-23 10:50\", \"Laden\", None),\n",
    "        ...         (\"A2\", \"2022-01-23 11:00\", \"Antrag (VE)\", \"\"),\n",
    "        ...         (\"A2\", \"2022-01-23 11:01\", \"Geloescht\", \"\")]\n",
    "        >>> columns = [\"CASE_KEY\", \"Datum\", \"Funktion\", \"Tarifname\"]\n",
    "        >>> df = spark.createDataFrame(data, schema=columns)\n",
    "        >>> result_df = find_case_keys_for_event_sequence(df, \"CASE_KEY\", \"Funktion\", \"Datum\", \"Antrag Start -> Fristablauf ext.\")\n",
    "        >>> result_df.show(truncate=False)\n",
    "    \"\"\"\n",
    "    # Validate that the specified columns exist in the DataFrame\n",
    "    for col_name in [case_key, event, timestamp]:\n",
    "        if col_name not in df.columns:\n",
    "            raise ValueError(\n",
    "                f\"The column '{col_name}' does not exist in the DataFrame\")\n",
    "\n",
    "    # Define the window specification\n",
    "    window = Window.partitionBy(case_key).orderBy(timestamp)\n",
    "\n",
    "    # Create a column for the next event in the sequence\n",
    "    df = df.withColumn(\"next_event\", F.lead(event).over(window))\n",
    "\n",
    "    # Filter out rows where next_event is null\n",
    "    df_event_sequence = df.filter(F.col(\"next_event\").isNotNull())\n",
    "\n",
    "    # Create a column for the event sequence\n",
    "    df_event_sequence = df_event_sequence.withColumn(\n",
    "        \"event_sequence\", F.concat(\n",
    "            F.col(event), F.lit(\" -> \"), F.col(\"next_event\"))\n",
    "    )\n",
    "\n",
    "    # Filter for the specified event sequence and select distinct case keys\n",
    "    df_case_keys_for_sequence = (\n",
    "        df_event_sequence.filter(F.col(\"event_sequence\") == sequence)\n",
    "        .select(case_key)\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    return df_case_keys_for_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_sequence_total(\n",
    "    df: DataFrame, case_key: str, event: str, timestamp: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the count and percentage of sequences over all events, starting from the first event until the last event, and return the result as a dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe.\n",
    "        case_key (str): The column name for the case key.\n",
    "        event (str): The column name for the event.\n",
    "        timestamp (str): The column name for the timestamp.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The dataframe with the percentage of the most frequent sequence over all events.\n",
    "\n",
    "    Example:\n",
    "        result_df = calculate_percentage_most_frequent_sequence_all_events(df, \"CASE_KEY\", \"Funktion\", \"Timestamp\")\n",
    "    \"\"\"\n",
    "    window = Window.partitionBy(case_key).orderBy(timestamp)\n",
    "\n",
    "    df = df.withColumn(\"next_event\", F.lead(event).over(window))\n",
    "\n",
    "    df_event_sequence = df.filter(F.col(\"next_event\").isNotNull())\n",
    "\n",
    "    df_event_sequence = df_event_sequence.withColumn(\n",
    "        \"event_sequence\", F.concat(F.col(event), F.lit(\" -> \"), F.col(\"next_event\"))\n",
    "    )\n",
    "\n",
    "    df_event_sequence = df_event_sequence.groupBy(case_key).agg(\n",
    "        F.collect_list(\"event_sequence\").alias(\"event_sequence\")\n",
    "    )\n",
    "\n",
    "    df_event_sequence = df_event_sequence.withColumn(\n",
    "        \"event_sequence\", F.concat_ws(\", \", \"event_sequence\")\n",
    "    )\n",
    "\n",
    "    df_most_frequent_sequence_all_events = (\n",
    "        df_event_sequence.groupBy(\"event_sequence\").count().orderBy(F.desc(\"count\"))\n",
    "    )\n",
    "\n",
    "    total_count = df_most_frequent_sequence_all_events.select(F.sum(\"count\")).first()[0]\n",
    "\n",
    "    df_most_frequent_sequence_all_events = (\n",
    "        df_most_frequent_sequence_all_events.withColumn(\n",
    "            \"percentage\", (F.col(\"count\") / total_count) * 100\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df_most_frequent_sequence_all_events\n",
    "\n",
    "\n",
    "event_sequence_total(df, \"case_key\", \"Funktion\", \"Datum\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_case_keys_for_event_sequence_total(\n",
    "    df: DataFrame, case_key: str, event: str, timestamp: str, sequence: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Find the case keys for a particular full sequence of events and return the result as a dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe.\n",
    "        case_key (str): The column name for the case key.\n",
    "        event (str): The column name for the event.\n",
    "        timestamp (str): The column name for the timestamp.\n",
    "        sequence (str): The sequence of events.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The dataframe with the case keys for the sequence.\n",
    "\n",
    "    Example:\n",
    "        result_df = find_case_keys_for_full_sequence(df, \"CASE_KEY\", \"Funktion\", \"Timestamp\", \"event1 -> event2, event2 -> event3\")\n",
    "    \"\"\"\n",
    "    window = Window.partitionBy(case_key).orderBy(timestamp)\n",
    "\n",
    "    df = df.withColumn(\"next_event\", F.lead(event).over(window))\n",
    "\n",
    "    df_event_sequence = df.filter(F.col(\"next_event\").isNotNull())\n",
    "\n",
    "    df_event_sequence = df_event_sequence.withColumn(\n",
    "        \"event_sequence\", F.concat(F.col(event), F.lit(\" -> \"), F.col(\"next_event\"))\n",
    "    )\n",
    "\n",
    "    df_event_sequence = df_event_sequence.groupBy(case_key).agg(\n",
    "        F.collect_list(\"event_sequence\").alias(\"event_sequence\")\n",
    "    )\n",
    "\n",
    "    df_event_sequence = df_event_sequence.withColumn(\n",
    "        \"event_sequence\", F.concat_ws(\", \", \"event_sequence\")\n",
    "    )\n",
    "\n",
    "    df_case_keys_for_sequence = df_event_sequence.filter(\n",
    "        F.col(\"event_sequence\") == sequence\n",
    "    ).select(case_key)\n",
    "\n",
    "    return df_case_keys_for_sequence\n",
    "\n",
    "\n",
    "sequence = \"Antrag Start -> Sync, Sync -> Laden, Laden -> Antrag (VE), Antrag (VE) -> Geloescht\"\n",
    "find_case_keys_for_event_sequence_total(\n",
    "    df, \"CASE_KEY\", \"Funktion\", \"Datum\", sequence\n",
    ").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
