{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 14:11:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|   Name|Subject|Score|\n",
      "+-------+-------+-----+\n",
      "|  Alice|   Math|   85|\n",
      "|  Alice|English|   78|\n",
      "|    Bob|   Math|   92|\n",
      "|    Bob|English|   83|\n",
      "|Charlie|   Math|   79|\n",
      "|Charlie|English|   82|\n",
      "+-------+-------+-----+\n",
      "\n",
      "+-------+-----------+-------------+\n",
      "|   Name|Total_Score|Average_Score|\n",
      "+-------+-----------+-------------+\n",
      "|  Alice|        163|         81.5|\n",
      "|    Bob|        175|         87.5|\n",
      "|Charlie|        161|         80.5|\n",
      "+-------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ListComprehensionsExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Alice\", \"Math\", 85),\n",
    "    (\"Alice\", \"English\", 78),\n",
    "    (\"Bob\", \"Math\", 92),\n",
    "    (\"Bob\", \"English\", 83),\n",
    "    (\"Charlie\", \"Math\", 79),\n",
    "    (\"Charlie\", \"English\", 82),\n",
    "]\n",
    "\n",
    "# Creating DataFrame\n",
    "columns = [\"Name\", \"Subject\", \"Score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the original DataFrame\n",
    "df.show()\n",
    "\n",
    "# List of columns to aggregate\n",
    "columns_to_agg = [\"Score\"]\n",
    "\n",
    "# Aggregation functions\n",
    "agg_funcs = [(\"sum\", \"Total\"), (\"avg\", \"Average\")]\n",
    "\n",
    "# Mapping of function names to actual PySpark functions\n",
    "func_mapping = {\"sum\": sum, \"avg\": avg}\n",
    "\n",
    "# Generate aggregation expressions using list comprehension\n",
    "agg_exprs = [\n",
    "    func_mapping[func](col(col_name)).alias(f\"{prefix}_{col_name}\")\n",
    "    for col_name in columns_to_agg\n",
    "    for func, prefix in agg_funcs\n",
    "]\n",
    "\n",
    "# Group by and aggregate with dynamic columns and functions\n",
    "dynamic_grouped_df = df.groupBy(\"Name\").agg(*agg_exprs)\n",
    "\n",
    "# Show the dynamically aggregated DataFrame\n",
    "dynamic_grouped_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ListComprehensionsExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Alice\", \"Math\", 85), (\"Bob\", \"English\", 78), (\"Charlie\", \"Math\", 92)]\n",
    "columns = [\"Name\", \"Subject\", \"Score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# List of columns to select\n",
    "columns_to_select = [\"Name\", \"Score\"]\n",
    "\n",
    "# Using list comprehension to select columns\n",
    "selected_df = df.select([col(column) for column in columns_to_select])\n",
    "selected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"RenameValuesExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Alice\", \"Math\", 85), (\"Bob\", \"English\", 78), (\"Charlie\", \"Math\", 92)]\n",
    "columns = [\"Name\", \"Subject\", \"Score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the original DataFrame\n",
    "df.show()\n",
    "\n",
    "# Dictionary for mapping old values to new values\n",
    "subject_mapping = {\"Math\": \"Mathematics\", \"English\": \"English Literature\"}\n",
    "\n",
    "# 1. Using Dictionary Mapping\n",
    "renamed_df = df.withColumn(\n",
    "    \"RenamedSubject\",\n",
    "    when(col(\"Subject\").isin(subject_mapping.keys()), col(\"Subject\")).otherwise(\n",
    "        \"Other\"\n",
    "    ),\n",
    ")\n",
    "for old_val, new_val in subject_mapping.items():\n",
    "    renamed_df = renamed_df.withColumn(\n",
    "        \"RenamedSubject\",\n",
    "        when(col(\"RenamedSubject\") == old_val, new_val).otherwise(\n",
    "            col(\"RenamedSubject\")\n",
    "        ),\n",
    "    )\n",
    "renamed_df.show()\n",
    "\n",
    "# 2. Using List Comprehensions\n",
    "prefix = \"Subject: \"\n",
    "prefixed_df = df.withColumn(\n",
    "    \"PrefixedSubject\",\n",
    "    col(\"Subject\").rlike(\"|\".join(subject_mapping.keys())).alias(\"PrefixedSubject\"),\n",
    ").withColumn(\n",
    "    \"PrefixedSubject\",\n",
    "    when(col(\"PrefixedSubject\") == True, prefix + col(\"Subject\")).otherwise(\n",
    "        col(\"Subject\")\n",
    "    ),\n",
    ")\n",
    "prefixed_df.show()\n",
    "\n",
    "# 3. Using Custom Functions (UDF)\n",
    "\n",
    "\n",
    "def rename_subject(subject):\n",
    "    if subject in subject_mapping:\n",
    "        return subject_mapping[subject]\n",
    "    else:\n",
    "        return subject\n",
    "\n",
    "\n",
    "rename_subject_udf = udf(rename_subject, StringType())\n",
    "udf_renamed_df = df.withColumn(\n",
    "    \"RenamedSubject\", rename_subject_udf(col(\"Subject\")))\n",
    "udf_renamed_df.show()\n",
    "\n",
    "# 4. Using replace Method\n",
    "replace_renamed_df = df.replace(to_replace=subject_mapping, subset=[\"Subject\"])\n",
    "replace_renamed_df.show()\n",
    "\n",
    "# Stop the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10\n",
    "\n",
    "# List of columns to check against the threshold\n",
    "columns_to_check = [\n",
    "    col_name for col_name in df_event_count.columns if col_name != \"CASE_KEY\"\n",
    "]\n",
    "\n",
    "# Create new columns with the condition using list comprehension\n",
    "conditional_columns = [\n",
    "    (F.when(F.col(f\"`{column}`\") > threshold, 1).otherwise(0)).alias(\n",
    "        f\"{column}_gt_{threshold}\"\n",
    "    )\n",
    "    for column in columns_to_check\n",
    "]\n",
    "\n",
    "# Select original columns and new conditional columns\n",
    "result_df = df_event_count.select(\"*\", *conditional_columns)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"Alice\", 34, \"F\"), (\"Bob\", 45, \"M\")]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\", \"gender\"])\n",
    "\n",
    "# Create a map column from 'age' and 'gender'\n",
    "df_with_map = df.withColumn(\n",
    "    \"info_map\",\n",
    "    F.create_map(F.lit(\"age\"), F.col(\"age\"), F.lit(\"gender\"), F.col(\"gender\")),\n",
    ")\n",
    "\n",
    "df_with_map.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-------------+------------+----------+\n",
      "| id| name|score|score_plus_10|double_score|is_passing|\n",
      "+---+-----+-----+-------------+------------+----------+\n",
      "|  1|Alice|   50|           60|         100|      true|\n",
      "|  2|  Bob|   45|           55|          90|      true|\n",
      "+---+-----+-----+-------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"WithColumnListComprehension\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "df = spark.createDataFrame([(1, \"Alice\", 50), (2, \"Bob\", 45)], [\n",
    "                           \"id\", \"name\", \"score\"])\n",
    "\n",
    "# List of new column expressions\n",
    "new_columns = [\n",
    "    (\"score_plus_10\", col(\"score\") + 10),\n",
    "    (\"double_score\", col(\"score\") * 2),\n",
    "    (\"is_passing\", col(\"score\") > 40),\n",
    "]\n",
    "\n",
    "# Creating a new DataFrame with additional columns using list comprehension\n",
    "df_with_columns = df.select(\n",
    "    \"*\", *[col_expr.alias(col_name) for col_name, col_expr in new_columns]\n",
    ")\n",
    "\n",
    "df_with_columns.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-------------+------------+----------+\n",
      "| id| name|score|score_plus_10|double_score|is_passing|\n",
      "+---+-----+-----+-------------+------------+----------+\n",
      "|  1|Alice|   50|           60|         100|      true|\n",
      "|  2|  Bob|   45|           55|          90|      true|\n",
      "+---+-----+-----+-------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WithColumnsDictionary\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "df = spark.createDataFrame([(1, \"Alice\", 50), (2, \"Bob\", 45)], [\n",
    "                           \"id\", \"name\", \"score\"])\n",
    "\n",
    "# Dictionary mapping new column names to expressions\n",
    "new_columns_dict = {\n",
    "    \"score_plus_10\": col(\"score\") + 10,\n",
    "    \"double_score\": col(\"score\") * 2,\n",
    "    \"is_passing\": col(\"score\") > 40,\n",
    "}\n",
    "\n",
    "# Add multiple columns using withColumn in combination with a dictionary\n",
    "for col_name, col_expr in new_columns_dict.items():\n",
    "    df = df.withColumn(col_name, col_expr)\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"FilterMultipleConditionsVariables\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"Alice\", 50, \"HR\"),\n",
    "        (2, \"Bob\", 45, \"IT\"),\n",
    "        (3, \"Charlie\", 60, \"Finance\"),\n",
    "        (4, \"David\", 40, \"HR\"),\n",
    "    ],\n",
    "    [\"id\", \"name\", \"score\", \"department\"],\n",
    ")\n",
    "\n",
    "# Define conditions in variables\n",
    "condition1 = col(\"score\") > 40\n",
    "condition2 = col(\"score\") < 60\n",
    "condition3 = col(\"department\") == \"HR\"\n",
    "\n",
    "# Combine conditions using '&' (and)\n",
    "combined_condition = condition1 & condition2 & condition3\n",
    "\n",
    "# Apply the combined condition to filter the DataFrame\n",
    "filtered_df = df.filter(combined_condition)\n",
    "\n",
    "filtered_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"FilterMultipleConditionsDictionary\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"Alice\", 50, \"HR\"),\n",
    "        (2, \"Bob\", 45, \"IT\"),\n",
    "        (3, \"Charlie\", 60, \"Finance\"),\n",
    "        (4, \"David\", 40, \"HR\"),\n",
    "    ],\n",
    "    [\"id\", \"name\", \"score\", \"department\"],\n",
    ")\n",
    "\n",
    "# Define conditions in a dictionary\n",
    "conditions = {\n",
    "    \"score_gt_40\": col(\"score\") > 40,\n",
    "    \"score_lt_60\": col(\"score\") < 60,\n",
    "    \"department_hr\": col(\"department\") == \"HR\",\n",
    "}\n",
    "\n",
    "# Combine conditions using '&' (and)\n",
    "combined_condition = (\n",
    "    conditions[\"score_gt_40\"] & conditions[\"score_lt_60\"] & conditions[\"department_hr\"]\n",
    ")\n",
    "\n",
    "# Apply the combined condition to filter the DataFrame\n",
    "filtered_df = df.filter(combined_condition)\n",
    "\n",
    "filtered_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"NestedConditions\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"Alice\", 50, \"HR\"),\n",
    "        (2, \"Bob\", 45, \"IT\"),\n",
    "        (3, \"Charlie\", 60, \"Finance\"),\n",
    "        (4, \"David\", 40, \"HR\"),\n",
    "        (5, \"Eve\", 30, \"IT\"),\n",
    "    ],\n",
    "    [\"id\", \"name\", \"score\", \"department\"],\n",
    ")\n",
    "\n",
    "# Nested conditions\n",
    "condition = (col(\"score\") > 40) & (\n",
    "    (col(\"name\") == \"Alice\") | (col(\"department\") == \"HR\")\n",
    ")\n",
    "\n",
    "# Apply the nested condition to filter the DataFrame\n",
    "filtered_df = df.filter(condition)\n",
    "\n",
    "filtered_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ComplexNestedConditions\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"Alice\", 50, \"HR\"),\n",
    "        (2, \"Bob\", 45, \"IT\"),\n",
    "        (3, \"Charlie\", 60, \"Finance\"),\n",
    "        (4, \"David\", 40, \"HR\"),\n",
    "        (5, \"Eve\", 30, \"IT\"),\n",
    "    ],\n",
    "    [\"id\", \"name\", \"score\", \"department\"],\n",
    ")\n",
    "\n",
    "# Complex nested conditions\n",
    "condition = (\n",
    "    ((col(\"score\") > 40) & (col(\"score\") < 60)) | (\n",
    "        col(\"department\") == \"Finance\")\n",
    ") & (col(\"name\") != \"David\")\n",
    "\n",
    "# Apply the complex nested condition to filter the DataFrame\n",
    "filtered_df = df.filter(condition)\n",
    "\n",
    "filtered_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"DictionaryNestedConditions\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"Alice\", 50, \"HR\"),\n",
    "        (2, \"Bob\", 45, \"IT\"),\n",
    "        (3, \"Charlie\", 60, \"Finance\"),\n",
    "        (4, \"David\", 40, \"HR\"),\n",
    "        (5, \"Eve\", 30, \"IT\"),\n",
    "    ],\n",
    "    [\"id\", \"name\", \"score\", \"department\"],\n",
    ")\n",
    "\n",
    "# Define conditions in a dictionary\n",
    "conditions = {\n",
    "    \"score_between_40_60\": (col(\"score\") > 40) & (col(\"score\") < 60),\n",
    "    \"department_finance\": col(\"department\") == \"Finance\",\n",
    "    \"name_not_david\": col(\"name\") != \"David\",\n",
    "}\n",
    "\n",
    "# Combine conditions using logical operators\n",
    "combined_condition = (\n",
    "    conditions[\"score_between_40_60\"] | conditions[\"department_finance\"]\n",
    ") & conditions[\"name_not_david\"]\n",
    "\n",
    "# Apply the combined condition to filter the DataFrame\n",
    "filtered_df = df.filter(combined_condition)\n",
    "\n",
    "filtered_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
