{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/03 09:02:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/03 09:02:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/06/03 09:02:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from functools import reduce\n",
    "from typing import List, Union\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"Raw/data.csv\", header=True, inferSchema=False, sep=\";\")\n",
    "df = df.withColumn(\"Datum\", F.to_timestamp(\"Datum\", \"d.M.yy h:mm\"))\n",
    "# drop cases with missing case_key\n",
    "df = df.filter(F.col(\"case_key\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------------+---------+---------------+--------------------------------------+\n",
      "|CASE_KEY|Datum              |Funktion                 |Tarifname|map_df_CASE_KEY|Info                                  |\n",
      "+--------+-------------------+-------------------------+---------+---------------+--------------------------------------+\n",
      "|A1      |2022-01-23 08:00:00|Antrag Start             |null     |A1             |[Additional Info 1, Additional Info 2]|\n",
      "|A1      |2022-01-23 08:10:00|Fristablauf ext. Signatur|BU       |A1             |[Additional Info 1, Additional Info 2]|\n",
      "+--------+-------------------+-------------------------+---------+---------------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def enrich_data_with_mapping(\n",
    "    main_df: DataFrame,\n",
    "    map_df: DataFrame,\n",
    "    main_keys: Union[str, List[str]],\n",
    "    map_keys: Union[str, List[str]],\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Enriches the main_df by joining it with map_df on the provided keys.\n",
    "    If map_keys are not unique, combines all different values in the columns using collect_list.\n",
    "\n",
    "    Parameters:\n",
    "    main_df (DataFrame): The main dataframe.\n",
    "    map_df (DataFrame): The dataframe with additional information.\n",
    "    main_keys (Union[str, List[str]]): The key(s) to join main_df on.\n",
    "    map_keys (Union[str, List[str]]): The key(s) to join map_df on.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The enriched dataframe with additional information from map_df.\n",
    "    \"\"\"\n",
    "    # Ensure keys are lists\n",
    "    if isinstance(main_keys, str):\n",
    "        main_keys = [main_keys]\n",
    "    if isinstance(map_keys, str):\n",
    "        map_keys = [map_keys]\n",
    "\n",
    "    # Check that the keys exist in their respective dataframes\n",
    "    for key in main_keys:\n",
    "        if key not in main_df.columns:\n",
    "            raise ValueError(f\"Key '{key}' not found in main_df columns\")\n",
    "    for key in map_keys:\n",
    "        if key not in map_df.columns:\n",
    "            raise ValueError(f\"Key '{key}' not found in map_df columns\")\n",
    "\n",
    "    # Check for uniqueness in map_df using window function\n",
    "    map_df_grouped = map_df.groupBy(map_keys).count()\n",
    "    has_duplicates = map_df_grouped.filter(F.col(\"count\") > 1).count() > 0\n",
    "\n",
    "    if has_duplicates:\n",
    "        # Aggregate map_df to handle non-unique keys using collect_list\n",
    "        agg_exprs = [\n",
    "            F.collect_list(col).alias(col)\n",
    "            for col in map_df.columns\n",
    "            if col not in map_keys\n",
    "        ]\n",
    "        map_df_aggregated = map_df.groupBy(map_keys).agg(*agg_exprs)\n",
    "    else:\n",
    "        map_df_aggregated = map_df\n",
    "\n",
    "    # Alias the map_df columns to avoid ambiguity\n",
    "    map_df_aliased = map_df_aggregated.select(\n",
    "        *[F.col(c).alias(f\"map_df_{c}\") for c in map_df_aggregated.columns]\n",
    "    )\n",
    "    map_keys_aliased = [f\"map_df_{key}\" for key in map_keys]\n",
    "\n",
    "    # Create join conditions\n",
    "    join_condition = reduce(\n",
    "        lambda x, y: x & y,\n",
    "        [\n",
    "            F.col(main_keys[i]) == F.col(map_keys_aliased[i])\n",
    "            for i in range(len(main_keys))\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Select columns to join, avoiding ambiguity\n",
    "    main_df_columns = main_df.columns\n",
    "    map_df_columns = [\n",
    "        F.col(f\"map_df_{c}\").alias(c)\n",
    "        for c in map_df_aggregated.columns\n",
    "        if c not in main_df_columns\n",
    "    ]\n",
    "\n",
    "    # Broadcast the map_df to improve performance\n",
    "    map_df_broadcast = broadcast(\n",
    "        map_df_aliased.select(map_keys_aliased + map_df_columns)\n",
    "    )\n",
    "\n",
    "    # Perform the join\n",
    "    enriched_df = main_df.join(map_df_broadcast, join_condition, \"left\")\n",
    "\n",
    "    return enriched_df\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"ProcessMining\").getOrCreate()\n",
    "\n",
    "    # Example dataframes creation\n",
    "    data = [\n",
    "        (\"A1\", \"2022-01-23 08:00:00\", \"Antrag Start\", None),\n",
    "        (\"A1\", \"2022-01-23 08:10:00\", \"Fristablauf ext. Signatur\", \"BU\"),\n",
    "    ]\n",
    "    schema = [\"CASE_KEY\", \"Datum\", \"Funktion\", \"Tarifname\"]\n",
    "    main_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "    map_data = [\n",
    "        (\"A1\", \"2022-01-23 08:00:00\", \"Additional Info 1\"),\n",
    "        (\"A1\", \"2022-01-23 08:10:00\", \"Additional Info 2\"),\n",
    "        (\"A2\", \"2022-01-23 08:00:00\", \"Additional Info 3\"),\n",
    "    ]\n",
    "    map_schema = [\"CASE_KEY\", \"Datum\", \"Info\"]\n",
    "    map_df = spark.createDataFrame(map_data, map_schema)\n",
    "\n",
    "    enriched_df = enrich_data_with_mapping(main_df, map_df, [\"CASE_KEY\"], [\"CASE_KEY\"])\n",
    "    enriched_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
