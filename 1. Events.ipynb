{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"Raw/data.csv\", header=True, inferSchema=False, sep=\";\")\n",
    "df = df.withColumn(\"Datum\", F.to_timestamp(\"Datum\", \"d.M.yy h:mm\"))\n",
    "# drop cases with missing case_key\n",
    "df = df.filter(F.col(\"case_key\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_event_overview(\n",
    "    df: DataFrame, case_column: str, event_column: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the count of each event for each case and return the result as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        case_column (str): The column name for the case key.\n",
    "        event_column (str): The column name for the event.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with the count of each event for each case key.\n",
    "\n",
    "    Example:\n",
    "        result_df = count_events_per_case(df, \"CASE_KEY\", \"Funktion\")\n",
    "    \"\"\"\n",
    "    # Group by the case column and pivot on the event column, counting occurrences\n",
    "    event_counts = df.groupBy(case_column).pivot(event_column).count().na.fill(0)\n",
    "    return event_counts\n",
    "\n",
    "\n",
    "result_df = case_event_overview(df, \"case_key\", \"Funktion\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_event_count(df: DataFrame, case_key: str, event: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the event count and the number of distinct events for each case key, order the results in descending order by the number of events, and return the result as a dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe.\n",
    "        case_key (str): The column name for the case key.\n",
    "        event (str): The column name for the event.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The dataframe with the count of each event and the number of distinct events for each case key, ordered by the number of events in descending order.\n",
    "\n",
    "    Example:\n",
    "        example_data = [(\"CASE1\", \"Event1\"), (\"CASE1\", \"Event2\"), (\"CASE2\", \"Event1\"), (\"CASE1\", \"Event1\")]\n",
    "        example_df = spark.createDataFrame(example_data, [\"CASE_KEY\", \"Funktion\"])\n",
    "        result_df = calculate_number_of_events_per_case(example_df, \"CASE_KEY\", \"Funktion\")\n",
    "        result_df.show()\n",
    "    \"\"\"\n",
    "    df_event_count_per_case = df.groupBy(case_key).agg(\n",
    "        F.count(event).alias(\"Number of Events\"),\n",
    "        F.countDistinct(event).alias(\"Number of Distinct Events\"),\n",
    "    )\n",
    "    df_event_count_per_case = df_event_count_per_case.orderBy(\n",
    "        F.desc(\"Number of Events\")\n",
    "    )\n",
    "    return df_event_count_per_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_count(df: DataFrame, case_key: str, event: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Count the number of distinct cases and the total number of events for each case and return the result as a dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe.\n",
    "        case_key (str): The column name for the case key.\n",
    "        event (str): The column name for the event.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The dataframe with the count of distinct cases and the total number of events for each case.\n",
    "\n",
    "    Example:\n",
    "        example_data = [(\"CASE1\", \"Event1\"), (\"CASE1\", \"Event2\"), (\"CASE2\", \"Event1\"), (\"CASE1\", \"Event1\")]\n",
    "        example_df = spark.createDataFrame(example_data, [\"CASE_KEY\", \"Funktion\"])\n",
    "        result_df = count_cases_and_events(example_df, \"CASE_KEY\", \"Funktion\")\n",
    "        result_df.show()\n",
    "    \"\"\"\n",
    "    df_cases_and_events = df.groupBy(event).agg(\n",
    "        F.countDistinct(case_key).alias(\"Number of Distinct Cases\"),\n",
    "        F.count(case_key).alias(\"Total Number Cases\"),\n",
    "    )\n",
    "    df_cases_and_events = df_cases_and_events.withColumn(\n",
    "        \"Iterations\", F.col(\"Total Number Cases\") - F.col(\"Number of Distinct Cases\")\n",
    "    )\n",
    "    df_cases_and_events = df_cases_and_events.orderBy(\n",
    "        F.col(\"Number of Distinct Cases\").desc()\n",
    "    )\n",
    "    return df_cases_and_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_percentage(df: DataFrame, case_key: str, event: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the percentage of cases that had each event, order the results in descending order, and return the result as a dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe.\n",
    "        case_key (str): The column name for the case key.\n",
    "        event (str): The column name for the event.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The dataframe with the percentage of cases for each event, ordered in descending order.\n",
    "\n",
    "    Example:\n",
    "        example_data = [(\"CASE1\", \"Event1\"), (\"CASE1\", \"Event2\"), (\"CASE2\", \"Event1\"), (\"CASE1\", \"Event1\")]\n",
    "        example_df = spark.createDataFrame(example_data, [\"CASE_KEY\", \"Funktion\"])\n",
    "        result_df = calculate_event_percentage(example_df, \"CASE_KEY\", \"Funktion\")\n",
    "        result_df.show()\n",
    "    \"\"\"\n",
    "    total_cases = df.select(case_key).distinct().count()\n",
    "    df_event_percentage = df.groupBy(event).agg(\n",
    "        (F.countDistinct(case_key) / total_cases * 100).alias(\n",
    "            \"Percentage of Distinct Cases\"\n",
    "        ),\n",
    "        (F.count(case_key) / total_cases * 100).alias(\"Percentage of Cases\"),\n",
    "    )\n",
    "    df_event_percentage = df_event_percentage.orderBy(F.desc(\"Percentage of Cases\"))\n",
    "    return df_event_percentage\n",
    "\n",
    "\n",
    "event_percentage(df, \"case_key\", \"Funktion\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_start_end_percentage(\n",
    "    df: DataFrame, case_key: str, event: str, timestamp: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the percentage of case keys that start with an event and the percentage that end with an event, order the results by the highest percentage, and return the result as a dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe.\n",
    "        case_key (str): The column name for the case key.\n",
    "        event (str): The column name for the event.\n",
    "        timestamp (str): The column name for the timestamp.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The dataframe with the percentage of case keys that start with an event and the percentage that end with an event, ordered by the highest percentage.\n",
    "\n",
    "    Example:\n",
    "        example_data = [(\"CASE1\", \"Event1\", \"2022-01-01\"), (\"CASE1\", \"Event2\", \"2022-01-02\"), (\"CASE2\", \"Event1\", \"2022-01-01\"), (\"CASE1\", \"Event1\", \"2022-01-03\")]\n",
    "        example_df = spark.createDataFrame(example_data, [\"CASE_KEY\", \"Funktion\", \"Timestamp\"])\n",
    "        result_df = calculate_start_end_event_percentage(example_df, \"CASE_KEY\", \"Funktion\", \"Timestamp\")\n",
    "        result_df.show()\n",
    "    \"\"\"\n",
    "    window = Window.partitionBy(case_key).orderBy(timestamp)\n",
    "    df_start_end_events = df.withColumn(\"Start Event\", F.first(event).over(window))\n",
    "    df_start_end_events = df_start_end_events.withColumn(\n",
    "        \"End Event\", F.last(event).over(window)\n",
    "    )\n",
    "\n",
    "    df_start_end_events = df_start_end_events.groupby(case_key).agg(\n",
    "        F.first(\"Start Event\").alias(\"Start Event\"),\n",
    "        F.last(\"End Event\").alias(\"End Event\"),\n",
    "    )\n",
    "\n",
    "    total_cases = df.select(case_key).distinct().count()\n",
    "\n",
    "    df_start_event_percentage = (\n",
    "        df_start_end_events.groupBy(\"Start Event\")\n",
    "        .agg(\n",
    "            (F.countDistinct(case_key) / total_cases * 100).alias(\n",
    "                \"Percentage of Cases Starting\"\n",
    "            )\n",
    "        )\n",
    "        .orderBy(F.desc(\"Percentage of Cases Starting\"))\n",
    "    )\n",
    "\n",
    "    df_end_event_percentage = (\n",
    "        df_start_end_events.groupBy(\"End Event\")\n",
    "        .agg(\n",
    "            (F.countDistinct(case_key) / total_cases * 100).alias(\n",
    "                \"Percentage of Cases Ending\"\n",
    "            )\n",
    "        )\n",
    "        .orderBy(F.desc(\"Percentage of Cases Ending\"))\n",
    "    )\n",
    "\n",
    "    return df_start_event_percentage.join(\n",
    "        df_end_event_percentage,\n",
    "        df_start_event_percentage[\"Start Event\"]\n",
    "        == df_end_event_percentage[\"End Event\"],\n",
    "        \"outer\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
