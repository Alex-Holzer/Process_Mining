{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from functools import reduce\n",
    "from typing import List, Union\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"Raw/data.csv\", header=True, inferSchema=False, sep=\";\")\n",
    "df = df.withColumn(\"Datum\", F.to_timestamp(\"Datum\", \"d.M.yy h:mm\"))\n",
    "# drop cases with missing case_key\n",
    "df = df.filter(F.col(\"case_key\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter cases with event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cases_by_events(\n",
    "    df: DataFrame,\n",
    "    case_key: str,\n",
    "    event: str,\n",
    "    contain_events: List[str],\n",
    "    all_events_required: bool = False,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include only those case_keys that have at least one event\n",
    "    matching any of the events in the contain_events list, or all events if specified.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the process data.\n",
    "    case_key (str): The column name for case keys.\n",
    "    event (str): The column name for events.\n",
    "    contain_events (List[str]): The list of events to filter by.\n",
    "    all_events_required (bool): If True, only include case_keys that contain all events in contain_events.\n",
    "                                If False, include case_keys that contain at least one event in contain_events.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The filtered DataFrame containing only the relevant case keys.\n",
    "\n",
    "    Example:\n",
    "    >>> df = spark.createDataFrame([\n",
    "    ...     (\"A1\", \"2023-01-23 08:00:00\", \"Antrag Start\"),\n",
    "    ...     (\"A1\", \"2023-01-23 08:10:00\", \"Fristablauf ext. Signatul BU\"),\n",
    "    ...     (\"A2\", \"2023-01-23 10:00:00\", \"Antrag Start\"),\n",
    "    ...     (\"A2\", \"2023-01-23 10:20:00\", \"Sync\"),\n",
    "    ...     (\"A2\", \"2023-01-23 11:00:00\", \"Antrag (VE)\"),\n",
    "    ... ], [\"CASE_KEY\", \"Datum\", \"Funktion\"])\n",
    "    >>> contain_events = [\"Antrag Start\", \"Sync\"]\n",
    "    >>> result = filter_cases_by_events(df, \"CASE_KEY\", \"Funktion\", contain_events, all_events_required=False)\n",
    "    >>> result.show()\n",
    "    +--------+-------------------+--------------------+\n",
    "    |CASE_KEY|              Datum|            Funktion|\n",
    "    +--------+-------------------+--------------------+\n",
    "    |      A1|2023-01-23 08:00:00|        Antrag Start|\n",
    "    |      A1|2023-01-23 08:10:00|Fristablauf ext. ...|\n",
    "    |      A2|2023-01-23 10:00:00|        Antrag Start|\n",
    "    |      A2|2023-01-23 10:20:00|                Sync|\n",
    "    |      A2|2023-01-23 11:00:00|         Antrag (VE)|\n",
    "    +--------+-------------------+--------------------+\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(df, DataFrame), \"df must be a pyspark.sql.DataFrame\"\n",
    "    assert case_key in df.columns, f\"Column '{case_key}' not found in DataFrame\"\n",
    "    assert event in df.columns, f\"Column '{event}' not found in DataFrame\"\n",
    "    assert isinstance(contain_events, list) and all(\n",
    "        isinstance(e, str) for e in contain_events\n",
    "    ), \"contain_events should be a list of strings\"\n",
    "\n",
    "    if all_events_required:\n",
    "\n",
    "        matching_cases = (\n",
    "            df.filter(F.col(event).isin(contain_events))\n",
    "            .groupBy(case_key)\n",
    "            .agg(F.collect_set(event).alias(\"events\"))\n",
    "            .filter(\n",
    "                F.size(\n",
    "                    F.array_intersect(\n",
    "                        F.col(\"events\"), F.array(*[F.lit(e) for e in contain_events])\n",
    "                    )\n",
    "                )\n",
    "                == len(contain_events)\n",
    "            )\n",
    "            .select(case_key)\n",
    "        )\n",
    "    else:\n",
    "        matching_cases = (\n",
    "            df.filter(F.col(event).isin(contain_events))\n",
    "            .select(case_key)\n",
    "            .dropDuplicates()\n",
    "        )\n",
    "\n",
    "    filtered_df = df.join(matching_cases, on=case_key, how=\"inner\")\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cases_by_start_end_events(\n",
    "    df: DataFrame,\n",
    "    case_key: str,\n",
    "    date_col: str,\n",
    "    event_col: str,\n",
    "    start_event: str,\n",
    "    end_event: str,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include only those case_keys that start with the start_event\n",
    "    and end with the end_event based on the date column.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the process data.\n",
    "    case_key (str): The column name for case keys.\n",
    "    date_col (str): The column name for dates.\n",
    "    event_col (str): The column name for events.\n",
    "    start_event (str): The event that should be the first event for a case.\n",
    "    end_event (str): The event that should be the last event for a case.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The filtered DataFrame containing only the relevant case keys.\n",
    "\n",
    "    Example:\n",
    "    >>> df = spark.createDataFrame([\n",
    "    ...     (\"A1\", \"2023-01-23 08:00:00\", \"Antrag Start\"),\n",
    "    ...     (\"A1\", \"2023-01-23 08:10:00\", \"Fristablauf ext. Signatul BU\"),\n",
    "    ...     (\"A1\", \"2023-01-23 09:00:00\", \"Antrag Vollstaendig\"),\n",
    "    ...     (\"A1\", \"2023-01-23 10:00:00\", \"Geloescht\"),\n",
    "    ...     (\"A2\", \"2023-01-23 10:00:00\", \"Antrag Start\"),\n",
    "    ...     (\"A2\", \"2023-01-23 10:20:00\", \"Sync\"),\n",
    "    ...     (\"A2\", \"2023-01-23 11:00:00\", \"Antrag (VE)\"),\n",
    "    ...     (\"A2\", \"2023-01-23 11:01:00\", \"Geloescht\"),\n",
    "    ... ], [\"CASE_KEY\", \"Datum\", \"Funktion\"])\n",
    "    >>> result = filter_cases_by_start_end_events(df, \"CASE_KEY\", \"Datum\", \"Funktion\", \"Antrag Start\", \"Geloescht\")\n",
    "    >>> result.show()\n",
    "    +--------+-------------------+--------------------+\n",
    "    |CASE_KEY|              Datum|            Funktion|\n",
    "    +--------+-------------------+--------------------+\n",
    "    |      A1|2023-01-23 08:00:00|        Antrag Start|\n",
    "    |      A1|2023-01-23 08:10:00|Fristablauf ext. ...|\n",
    "    |      A1|2023-01-23 09:00:00|  Antrag Vollstaendig|\n",
    "    |      A1|2023-01-23 10:00:00|             Geloescht|\n",
    "    |      A2|2023-01-23 10:00:00|        Antrag Start|\n",
    "    |      A2|2023-01-23 10:20:00|                Sync|\n",
    "    |      A2|2023-01-23 11:00:00|         Antrag (VE)|\n",
    "    |      A2|2023-01-23 11:01:00|             Geloescht|\n",
    "    +--------+-------------------+--------------------+\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate input parameters\n",
    "    assert isinstance(df, DataFrame), \"df must be a pyspark.sql.DataFrame\"\n",
    "    assert case_key in df.columns, f\"Column '{case_key}' not found in DataFrame\"\n",
    "    assert date_col in df.columns, f\"Column '{date_col}' not found in DataFrame\"\n",
    "    assert event_col in df.columns, f\"Column '{event_col}' not found in DataFrame\"\n",
    "\n",
    "    # Get the first event for each case_key\n",
    "    first_events = (\n",
    "        df.withColumn(\n",
    "            \"row_num\",\n",
    "            F.row_number().over(Window.partitionBy(case_key).orderBy(F.col(date_col))),\n",
    "        )\n",
    "        .filter(F.col(\"row_num\") == 1)\n",
    "        .filter(F.col(event_col) == start_event)\n",
    "        .select(case_key)\n",
    "    )\n",
    "\n",
    "    # Get the last event for each case_key\n",
    "    last_events = (\n",
    "        df.withColumn(\n",
    "            \"row_num\",\n",
    "            F.row_number().over(\n",
    "                Window.partitionBy(case_key).orderBy(F.col(date_col).desc())\n",
    "            ),\n",
    "        )\n",
    "        .filter(F.col(\"row_num\") == 1)\n",
    "        .filter(F.col(event_col) == end_event)\n",
    "        .select(case_key)\n",
    "    )\n",
    "\n",
    "    # Find the intersection of case_keys that have both the start_event and end_event\n",
    "    matching_cases = first_events.intersect(last_events)\n",
    "\n",
    "    # Perform an inner join to keep only the relevant case keys\n",
    "    filtered_df = df.join(matching_cases, on=case_key, how=\"inner\")\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cases_by_start_event(\n",
    "    df: DataFrame, case_key: str, date_col: str, event_col: str, start_event: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include only those case_keys that start with the start_event\n",
    "    based on the date column.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the process data.\n",
    "    case_key (str): The column name for case keys.\n",
    "    date_col (str): The column name for dates.\n",
    "    event_col (str): The column name for events.\n",
    "    start_event (str): The event that should be the first event for a case.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The filtered DataFrame containing only the relevant case keys.\n",
    "\n",
    "    Example:\n",
    "    >>> df = spark.createDataFrame([\n",
    "    ...     (\"A1\", \"2023-01-23 08:00:00\", \"Antrag Start\"),\n",
    "    ...     (\"A1\", \"2023-01-23 08:10:00\", \"Fristablauf ext. Signatul BU\"),\n",
    "    ...     (\"A1\", \"2023-01-23 09:00:00\", \"Antrag Vollstaendig\"),\n",
    "    ...     (\"A1\", \"2023-01-23 10:00:00\", \"Geloescht\"),\n",
    "    ...     (\"A2\", \"2023-01-23 10:00:00\", \"Antrag Start\"),\n",
    "    ...     (\"A2\", \"2023-01-23 10:20:00\", \"Sync\"),\n",
    "    ...     (\"A2\", \"2023-01-23 11:00:00\", \"Antrag (VE)\"),\n",
    "    ...     (\"A2\", \"2023-01-23 11:01:00\", \"Geloescht\"),\n",
    "    ... ], [\"CASE_KEY\", \"Datum\", \"Funktion\"])\n",
    "    >>> result = filter_cases_by_start_event(df, \"CASE_KEY\", \"Datum\", \"Funktion\", \"Antrag Start\")\n",
    "    >>> result.show()\n",
    "    +--------+-------------------+--------------------+\n",
    "    |CASE_KEY|              Datum|            Funktion|\n",
    "    +--------+-------------------+--------------------+\n",
    "    |      A1|2023-01-23 08:00:00|        Antrag Start|\n",
    "    |      A1|2023-01-23 08:10:00|Fristablauf ext. ...|\n",
    "    |      A1|2023-01-23 09:00:00|  Antrag Vollstaendig|\n",
    "    |      A1|2023-01-23 10:00:00|             Geloescht|\n",
    "    |      A2|2023-01-23 10:00:00|        Antrag Start|\n",
    "    |      A2|2023-01-23 10:20:00|                Sync|\n",
    "    |      A2|2023-01-23 11:00:00|         Antrag (VE)|\n",
    "    |      A2|2023-01-23 11:01:00|             Geloescht|\n",
    "    +--------+-------------------+--------------------+\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate input parameters\n",
    "    assert isinstance(df, DataFrame), \"df must be a pyspark.sql.DataFrame\"\n",
    "    assert case_key in df.columns, f\"Column '{case_key}' not found in DataFrame\"\n",
    "    assert date_col in df.columns, f\"Column '{date_col}' not found in DataFrame\"\n",
    "    assert event_col in df.columns, f\"Column '{event_col}' not found in DataFrame\"\n",
    "\n",
    "    # Get the first event for each case_key\n",
    "    window_spec = Window.partitionBy(case_key).orderBy(F.col(date_col))\n",
    "    first_events = (\n",
    "        df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"row_num\") == 1)\n",
    "        .filter(F.col(event_col) == start_event)\n",
    "        .select(case_key)\n",
    "    )\n",
    "\n",
    "    # Perform an inner join to keep only the relevant case keys\n",
    "    filtered_df = df.join(first_events, on=case_key, how=\"inner\")\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cases_by_end_event(\n",
    "    df: DataFrame, case_key: str, date_col: str, event_col: str, end_event: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include only those case_keys that end with the end_event\n",
    "    based on the date column.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the process data.\n",
    "    case_key (str): The column name for case keys.\n",
    "    date_col (str): The column name for dates.\n",
    "    event_col (str): The column name for events.\n",
    "    end_event (str): The event that should be the last event for a case.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The filtered DataFrame containing only the relevant case keys.\n",
    "\n",
    "    Example:\n",
    "    >>> df = spark.createDataFrame([\n",
    "    ...     (\"A1\", \"2023-01-23 08:00:00\", \"Antrag Start\"),\n",
    "    ...     (\"A1\", \"2023-01-23 08:10:00\", \"Fristablauf ext. Signatul BU\"),\n",
    "    ...     (\"A1\", \"2023-01-23 09:00:00\", \"Antrag Vollstaendig\"),\n",
    "    ...     (\"A1\", \"2023-01-23 10:00:00\", \"Geloescht\"),\n",
    "    ...     (\"A2\", \"2023-01-23 10:00:00\", \"Antrag Start\"),\n",
    "    ...     (\"A2\", \"2023-01-23 10:20:00\", \"Sync\"),\n",
    "    ...     (\"A2\", \"2023-01-23 11:00:00\", \"Antrag (VE)\"),\n",
    "    ...     (\"A2\", \"2023-01-23 11:01:00\", \"Geloescht\"),\n",
    "    ... ], [\"CASE_KEY\", \"Datum\", \"Funktion\"])\n",
    "    >>> result = filter_cases_by_end_event(df, \"CASE_KEY\", \"Datum\", \"Funktion\", \"Geloescht\")\n",
    "    >>> result.show()\n",
    "    +--------+-------------------+--------------------+\n",
    "    |CASE_KEY|              Datum|            Funktion|\n",
    "    +--------+-------------------+--------------------+\n",
    "    |      A1|2023-01-23 08:00:00|        Antrag Start|\n",
    "    |      A1|2023-01-23 08:10:00|Fristablauf ext. ...|\n",
    "    |      A1|2023-01-23 09:00:00|  Antrag Vollstaendig|\n",
    "    |      A1|2023-01-23 10:00:00|             Geloescht|\n",
    "    |      A2|2023-01-23 10:00:00|        Antrag Start|\n",
    "    |      A2|2023-01-23 10:20:00|                Sync|\n",
    "    |      A2|2023-01-23 11:00:00|         Antrag (VE)|\n",
    "    |      A2|2023-01-23 11:01:00|             Geloescht|\n",
    "    +--------+-------------------+--------------------+\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate input parameters\n",
    "    assert isinstance(df, DataFrame), \"df must be a pyspark.sql.DataFrame\"\n",
    "    assert case_key in df.columns, f\"Column '{case_key}' not found in DataFrame\"\n",
    "    assert date_col in df.columns, f\"Column '{date_col}' not found in DataFrame\"\n",
    "    assert event_col in df.columns, f\"Column '{event_col}' not found in DataFrame\"\n",
    "\n",
    "    # Get the last event for each case_key\n",
    "    window_spec = Window.partitionBy(case_key).orderBy(F.col(date_col).desc())\n",
    "    last_events = (\n",
    "        df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"row_num\") == 1)\n",
    "        .filter(F.col(event_col) == end_event)\n",
    "        .select(case_key)\n",
    "    )\n",
    "\n",
    "    # Perform an inner join to keep only the relevant case keys\n",
    "    filtered_df = df.join(last_events, on=case_key, how=\"inner\")\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cases_by_timeframe(\n",
    "    df: DataFrame, case_key: str, date_col: str, timeframe: str, time_duration: int = 1\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include only those case_keys that started and ended\n",
    "    within the defined timeframe.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the process data.\n",
    "    case_key (str): The column name for case keys.\n",
    "    date_col (str): The column name for dates.\n",
    "    timeframe (str): The timeframe to filter by ('hour', 'day', 'week', 'month', 'year').\n",
    "    time_duration (int): The duration of the timeframe to filter by (default is 1).\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The filtered DataFrame containing only the relevant case keys.\n",
    "\n",
    "    Example:\n",
    "    >>> df = spark.createDataFrame([\n",
    "    ...     (\"A1\", \"2023-01-23 08:00:00\"),\n",
    "    ...     (\"A1\", \"2023-01-23 08:10:00\"),\n",
    "    ...     (\"A1\", \"2023-01-23 09:00:00\"),\n",
    "    ...     (\"A1\", \"2023-01-23 10:00:00\"),\n",
    "    ...     (\"A2\", \"2023-01-23 10:00:00\"),\n",
    "    ...     (\"A2\", \"2023-01-23 10:20:00\"),\n",
    "    ...     (\"A2\", \"2023-01-23 11:00:00\"),\n",
    "    ...     (\"A2\", \"2023-01-23 11:01:00\"),\n",
    "    ... ], [\"CASE_KEY\", \"Datum\"])\n",
    "    >>> result = filter_cases_by_timeframe(df, \"CASE_KEY\", \"Datum\", \"hour\", 1)\n",
    "    >>> result.show()\n",
    "    +--------+-------------------+\n",
    "    |CASE_KEY|              Datum|\n",
    "    +--------+-------------------+\n",
    "    |      A2|2023-01-23 10:00:00|\n",
    "    |      A2|2023-01-23 10:20:00|\n",
    "    |      A2|2023-01-23 11:00:00|\n",
    "    |      A2|2023-01-23 11:01:00|\n",
    "    +--------+-------------------+\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate input parameters\n",
    "    assert isinstance(df, DataFrame), \"df must be a pyspark.sql.DataFrame\"\n",
    "    assert case_key in df.columns, f\"Column '{case_key}' not found in DataFrame\"\n",
    "    assert date_col in df.columns, f\"Column '{date_col}' not found in DataFrame\"\n",
    "    assert timeframe in [\n",
    "        \"hour\",\n",
    "        \"day\",\n",
    "        \"week\",\n",
    "        \"month\",\n",
    "        \"year\",\n",
    "    ], \"timeframe must be one of 'hour', 'day', 'week', 'month', 'year'\"\n",
    "\n",
    "    # Convert the date column to TimestampType if it is not already\n",
    "    df = df.withColumn(date_col, F.col(date_col).cast(TimestampType()))\n",
    "\n",
    "    # Calculate the first and last event times for each case_key\n",
    "    window_spec = Window.partitionBy(case_key).orderBy(F.col(date_col))\n",
    "    first_last_times = (\n",
    "        df.withColumn(\"first_event_time\", F.first(\n",
    "            F.col(date_col)).over(window_spec))\n",
    "        .withColumn(\"last_event_time\", F.last(F.col(date_col)).over(window_spec))\n",
    "        .select(case_key, \"first_event_time\", \"last_event_time\")\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # Define the time difference condition based on the timeframe\n",
    "    time_diff_cond = {\n",
    "        \"hour\": F.expr(\n",
    "            f\"first_event_time + INTERVAL {time_duration} HOUR >= last_event_time\"\n",
    "        ),\n",
    "        \"day\": F.expr(\n",
    "            f\"first_event_time + INTERVAL {time_duration} DAY >= last_event_time\"\n",
    "        ),\n",
    "        \"week\": F.expr(\n",
    "            f\"first_event_time + INTERVAL {time_duration} WEEK >= last_event_time\"\n",
    "        ),\n",
    "        \"month\": F.expr(\n",
    "            f\"first_event_time + INTERVAL {time_duration} MONTH >= last_event_time\"\n",
    "        ),\n",
    "        \"year\": F.expr(\n",
    "            f\"first_event_time + INTERVAL {time_duration} YEAR >= last_event_time\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Filter the case_keys that satisfy the timeframe condition\n",
    "    matching_cases = first_last_times.filter(\n",
    "        time_diff_cond[timeframe]).select(case_key)\n",
    "\n",
    "    # Perform an inner join to keep only the relevant case keys\n",
    "    filtered_df = df.join(matching_cases, on=case_key, how=\"inner\")\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cases_with_concurrent_events(\n",
    "    df: DataFrame, case_key: str, date_col: str, event_col: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include only those case_keys that have multiple events\n",
    "    occurring at the same exact timestamp.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the process data.\n",
    "    case_key (str): The column name for case keys.\n",
    "    date_col (str): The column name for dates.\n",
    "    event_col (str): The column name for events.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The filtered DataFrame containing only the relevant case keys.\n",
    "\n",
    "    Example:\n",
    "    >>> df = spark.createDataFrame([\n",
    "    ...     (\"A1\", \"2023-01-23 08:00:00\", \"Eingang\"),\n",
    "    ...     (\"A1\", \"2023-01-23 08:00:00\", \"Los\"),\n",
    "    ...     (\"A1\", \"2023-01-23 09:00:00\", \"Berechnung\"),\n",
    "    ...     (\"A2\", \"2023-01-23 10:00:00\", \"Eingang\"),\n",
    "    ...     (\"A2\", \"2023-01-23 11:00:00\", \"Berechnung\"),\n",
    "    ...     (\"A2\", \"2023-01-23 11:00:00\", \"Abschluss\"),\n",
    "    ...     (\"A2\", \"2023-01-23 11:00:00\", \"Versand\"),\n",
    "    ... ], [\"CASE_KEY\", \"Datum\", \"Funktion\"])\n",
    "    >>> result = filter_cases_with_concurrent_events(df, \"CASE_KEY\", \"Datum\", \"Funktion\")\n",
    "    >>> result.show()\n",
    "    +--------+-------------------+---------+\n",
    "    |CASE_KEY|              Datum| Funktion|\n",
    "    +--------+-------------------+---------+\n",
    "    |      A1|2023-01-23 08:00:00|  Eingang|\n",
    "    |      A1|2023-01-23 08:00:00|      Los|\n",
    "    |      A2|2023-01-23 11:00:00|Berechnung|\n",
    "    |      A2|2023-01-23 11:00:00|Abschluss|\n",
    "    |      A2|2023-01-23 11:00:00|  Versand|\n",
    "    +--------+-------------------+---------+\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate input parameters\n",
    "    assert isinstance(df, DataFrame), \"df must be a pyspark.sql.DataFrame\"\n",
    "    assert case_key in df.columns, f\"Column '{case_key}' not found in DataFrame\"\n",
    "    assert date_col in df.columns, f\"Column '{date_col}' not found in DataFrame\"\n",
    "    assert event_col in df.columns, f\"Column '{event_col}' not found in DataFrame\"\n",
    "\n",
    "    # Define the window specification to partition by case_key and date_col\n",
    "    window_spec = Window.partitionBy(case_key, date_col)\n",
    "\n",
    "    # Count the number of events at each timestamp for each case_key\n",
    "    df_with_counts = df.withColumn(\n",
    "        \"event_count\", F.count(event_col).over(window_spec))\n",
    "\n",
    "    # Filter the DataFrame to include only rows where the event_count is greater than 1\n",
    "    concurrent_events_df = df_with_counts.filter(F.col(\"event_count\") > 1)\n",
    "\n",
    "    # Drop the event_count column as it is no longer needed\n",
    "    result_df = concurrent_events_df.drop(\"event_count\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cases_by_start_and_next_event(\n",
    "    df: DataFrame,\n",
    "    case_key: str,\n",
    "    date_col: str,\n",
    "    event_col: str,\n",
    "    start_event: str,\n",
    "    next_event: str,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include only those case_keys that start with the start_event\n",
    "    and are directly followed by the next_event based on the date column.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the process data.\n",
    "    case_key (str): The column name for case keys.\n",
    "    date_col (str): The column name for dates.\n",
    "    event_col (str): The column name for events.\n",
    "    start_event (str): The event that should be the first event for a case.\n",
    "    next_event (str): The event that should directly follow the start_event for a case.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The filtered DataFrame containing only the relevant case keys.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate input parameters\n",
    "    assert isinstance(df, DataFrame), \"df must be a pyspark.sql.DataFrame\"\n",
    "    assert case_key in df.columns, f\"Column '{case_key}' not found in DataFrame\"\n",
    "    assert date_col in df.columns, f\"Column '{date_col}' not found in DataFrame\"\n",
    "    assert event_col in df.columns, f\"Column '{event_col}' not found in DataFrame\"\n",
    "\n",
    "    # Get the first event for each case_key\n",
    "    window_spec = Window.partitionBy(case_key).orderBy(F.col(date_col))\n",
    "    first_events = (\n",
    "        df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"row_num\") == 1)\n",
    "        .filter(F.col(event_col) == start_event)\n",
    "        .select(case_key)\n",
    "    )\n",
    "\n",
    "    # Get the second event for each case_key\n",
    "    second_events = (\n",
    "        df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"row_num\") == 2)\n",
    "        .filter(F.col(event_col) == next_event)\n",
    "        .select(case_key)\n",
    "    )\n",
    "\n",
    "    # Perform an inner join to keep only the relevant case keys\n",
    "    filtered_df = df.join(first_events, on=case_key, how=\"inner\").join(\n",
    "        second_events, on=case_key, how=\"inner\"\n",
    "    )\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cases_by_start_and_never_followed_event(\n",
    "    df: DataFrame,\n",
    "    case_key: str,\n",
    "    date_col: str,\n",
    "    event_col: str,\n",
    "    start_event: str,\n",
    "    never_event: str,\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include only those case_keys that start with the start_event\n",
    "    and are never followed by the never_event based on the date column.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the process data.\n",
    "    case_key (str): The column name for case keys.\n",
    "    date_col (str): The column name for dates.\n",
    "    event_col (str): The column name for events.\n",
    "    start_event (str): The event that should be the first event for a case.\n",
    "    never_event (str): The event that should never follow the start_event for a case.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The filtered DataFrame containing only the relevant case keys.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate input parameters\n",
    "    assert isinstance(df, DataFrame), \"df must be a pyspark.sql.DataFrame\"\n",
    "    assert case_key in df.columns, f\"Column '{case_key}' not found in DataFrame\"\n",
    "    assert date_col in df.columns, f\"Column '{date_col}' not found in DataFrame\"\n",
    "    assert event_col in df.columns, f\"Column '{event_col}' not found in DataFrame\"\n",
    "\n",
    "    # Define a window partitioned by case_key and ordered by date_col\n",
    "    window_spec = Window.partitionBy(case_key).orderBy(F.col(date_col))\n",
    "\n",
    "    # Add a column 'next_event' which shows the next event for each row\n",
    "    df = df.withColumn(\"next_event\", F.lead(event_col).over(window_spec))\n",
    "\n",
    "    # Filter rows where event_col is start_event and next_event is not never_event\n",
    "    filtered_df = df.filter(\n",
    "        (F.col(event_col) == start_event) & (\n",
    "            F.col(\"next_event\") != never_event)\n",
    "    )\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cases_by_never_flowed_through_event(\n",
    "    df: DataFrame, case_key: str, event_col: str, never_events: list\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include only those case_keys that never flowed through the never_events.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the process data.\n",
    "    case_key (str): The column name for case keys.\n",
    "    event_col (str): The column name for events.\n",
    "    never_events (list): The list of events that should never occur for a case.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The filtered DataFrame containing only the relevant case keys.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate input parameters\n",
    "    assert isinstance(df, DataFrame), \"df must be a pyspark.sql.DataFrame\"\n",
    "    assert case_key in df.columns, f\"Column '{case_key}' not found in DataFrame\"\n",
    "    assert event_col in df.columns, f\"Column '{event_col}' not found in DataFrame\"\n",
    "    assert isinstance(never_events, list), \"never_events must be a list\"\n",
    "    assert all(\n",
    "        isinstance(event, str) for event in never_events\n",
    "    ), \"All elements in never_events must be strings\"\n",
    "\n",
    "    # Create a DataFrame of cases that went through the never_events\n",
    "    never_df = (\n",
    "        df.filter(F.col(event_col).isin(never_events)).select(case_key).distinct()\n",
    "    )\n",
    "\n",
    "    # Perform a left anti join to find cases that never went through the never_events\n",
    "    filtered_df = df.join(never_df, on=case_key, how=\"left_anti\")\n",
    "\n",
    "    return filtered_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
